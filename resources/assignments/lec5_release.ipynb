{"cells":[{"cell_type":"markdown","metadata":{"id":"C6D3rHhQqAum"},"source":["# NETID: <fill in here\\>"]},{"cell_type":"markdown","source":["### Problems\n","- Problem 1 (2 points)\n","- Problem 2 (1 point)\n","- Problem 3 (4 points)\n","- Problem 4 (3 points)"],"metadata":{"id":"nuBRyHO14g_W"}},{"cell_type":"markdown","metadata":{"id":"ChDh8hwjtMVh"},"source":["# Assessing Model Accuracy\n","In this lesson, we look over different ways to evaluate whether machine learning models you have created successfully accomplish their intended objective."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H870tIe_tMWj","outputId":"8d361bc2-8d5d-40a4-f5e8-91f11eb89b7d","scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":408},"executionInfo":{"status":"error","timestamp":1697588724291,"user_tz":240,"elapsed":1775,"user":{"displayName":"Varun Gande","userId":"11529055052164094479"}}},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-f063d4840d34>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mweather\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lecture5dataA.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mnoncategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mweather\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Noncategorical Features: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoncategorical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lecture5dataA.csv'"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","from matplotlib import pyplot as plt\n","\n","weather = pd.read_csv('lecture5dataA.csv').dropna()\n","noncategorical = [weather.columns[i] for i in range (3,11) if i != 9]\n","print (\"Noncategorical Features: \", noncategorical)\n","weather.head()"]},{"cell_type":"markdown","metadata":{"id":"JWpTFF27tMXD"},"source":["## Loss Functions and Accuracy\n","\n","In evaluating your models, it's important to remember that different models must be evaluated with the appropriate metric. Classification accuracy is not, for example, the same thing as the mean-squared error used in regression problems. Furthermore, a high score in either of those metrics does not prove a model is \"good\"."]},{"cell_type":"markdown","metadata":{"id":"EvLepXLMqAut"},"source":["# Problem 1 (2 points)\n","Edit the lines marked with TODO's below to do the following:\n","1. Create two columns to `temperatures` to store Temperature and Apparent Temperature in Rankines. Rankines is a  weird unit of temperature. Temperature in Rankines is 9/5 * (temperature in Celcius) + 491.67.\n","2. Train and predict two models: one for celcius and one for rankines\n","3. Compare the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uJrP_FSTqAuu"},"outputs":[],"source":["temperatures = weather.loc[:,[\"Temperature (C)\", \"Apparent Temperature (C)\"]]\n","temperatures[\"Temperature (R)\"] = 0 #TODO: replace the 0 (hint: you don't need a loop)\n","temperatures[\"Apparent Temperature (R)\"] = 0 #TODO: replace the 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KtyH-s_FqAuw"},"outputs":[],"source":["celcius_model = LinearRegression()\n","# TODO split data. goal is temperature in celcius, feature is apparent temperature in celcius.\n","# Make sure that the names of your data for the two models are different! Otherwise, one will overwrite the other\n","# Then, fit the model.\n","\n","\n","x_tr_C, x_te_C, y_tr_C, y_te_C = # Fill in here\n","\n","\n","rankines_model = LinearRegression()\n","# TODO same as above, but for rankines\n","\n","\n","x_tr_R, x_te_R, y_tr_R, y_te_R = # Fill in here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGyhDcXBqAuy"},"outputs":[],"source":["from sklearn.metrics import mean_squared_error\n","\n","# TODO store the predictions for the test sets\n","celcius_predictions = \"Fill in here\"\n","rankines_predictions = \"Fill in here\"\n","\n","# TODO find mean squared error of each model's predictions\n","celcius_MSE = mean_squared_error(\"Fill in here\", \"Fill in here\") # TODO\n","rankines_MSE = mean_squared_error(\"Fill in here\", \"Fill in here\") # TODO\n","\n","print(\"celcius MSE:\", celcius_MSE)\n","print(\"rankines MSE:\", rankines_MSE)\n","print(\"\\n(if the MSE for rankines is 0, you missed something two cells above)\")"]},{"cell_type":"markdown","metadata":{"id":"mlIL6hKoqAuz"},"source":["#### The MSE's of the two models are significantly different -- one is more than triple the other. To inspect this difference, let's plot the predictions of the two models and compare."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aO0zdvI1qAu0"},"outputs":[],"source":["plt.subplots(figsize=(15, 5))\n","plt.subplot(121)\n","plt.scatter(x_te_C, y_te_C)\n","plt.plot(x_te_C, celcius_predictions, 'k', linewidth=4)\n","plt.legend([\"Predictions\",\"Actual Values\"])\n","plt.title('Celcius Linear Regression')\n","plt.xlabel('Apparent Temperature (C)')\n","plt.ylabel('Temperature (C)')\n","\n","plt.subplot(122)\n","plt.scatter(x_te_R, y_te_R)\n","plt.plot(x_te_R, rankines_predictions, 'k', linewidth=4)\n","plt.legend([\"Predictions\",\"Actual Values\"])\n","plt.title('Rankines Linear Regression')\n","plt.xlabel('Apparent Temperature (R)')\n","plt.ylabel('Temperature (R)')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"akGRKGNsqAu2"},"source":["#### The plots look the same! The only significant difference is the scale of the axes. That's why the MSE for Rankines is bigger: Rankines are generally greater than Celcius, and so their error is naturally bigger. To take care of this, we use a _baseline_."]},{"cell_type":"markdown","metadata":{"id":"JcSBtmN3qAu2"},"source":["### <span style=\"color:green\"><em>end of Problem 1</em></span>"]},{"cell_type":"markdown","source":["# Problem 2 (1 point)\n","Why do the plots look the same? How do the axes compare? Why is this what we expect?"],"metadata":{"id":"ht-PLaIa7-td"}},{"cell_type":"markdown","source":["fill in here"],"metadata":{"id":"EFdA4Mvo8LNI"}},{"cell_type":"markdown","metadata":{"id":"rRppePJSqAu3"},"source":["# Problem 3 (4 points)\n","\n","Compute the 'score' of the celcius model using sklearn's .score() method on *celcius_model*. Do the same for the Rankines model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3xIfk86AqAu3"},"outputs":[],"source":["print(\"sklearn's score for Celcius:\", celcius_model.score('FILL IN HERE','FILL IN HERE'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3EAL5ZmtqAu5"},"outputs":[],"source":["print(\"sklearn's score for Rankines:\", rankines_model.score('FILL IN HERE','FILL IN HERE'))"]},{"cell_type":"markdown","metadata":{"id":"vbkGaKLhNeBd"},"source":["But what exactly is .score() doing?\n","\n","When building a model, we typically have a baseline model to compare against. This allows us to see whether or not our model is better than a relatively simple, naive model.\n","\n","In our case, the most simple, naive (baseline) model we can build to predict a location's temperature in the test set is to simply predict the mean of all temperatures across the testing set (for every single test point in the test set).\n","\n","Go ahead and compute the MSE for the outputs of this baseline model: i.e. compute the MSE between the true outputs (i.e. *y_te_C*) and the predicted outputs of this baseline model (i.e. mean of the testing labels, *y_te_C*). Follow the same procedure for the Rankines model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOXPrezmqAu7"},"outputs":[],"source":["test_goal_mean_C = 'FILL IN HERE'\n","baseline_C = np.full((len(celcius_predictions),), test_goal_mean_C)\n","baseline_C_MSE = mean_squared_error('FILL IN HERE', 'FILL IN HERE')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xEaLf1qqqAu-"},"outputs":[],"source":["test_goal_mean_R = 'FILL IN HERE'\n","baseline_R = np.full((len(rankines_predictions),), test_goal_mean_R)\n","baseline_R_MSE = mean_squared_error('FILL IN HERE', 'FILL IN HERE')"]},{"cell_type":"markdown","metadata":{"id":"SflEPrARNeBe"},"source":["Now, compute the normalized score (relative to a baseline model) defined as: norm_score = 1 - model_MSE / baseline_MSE. If you did everything correctly, your computed normalized scores should be exactly the same as sklearn's .score() method.\n","\n","If necessary, ask TAs for help!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L0IAG2z2NeBf"},"outputs":[],"source":["score_C = 'FILL IN HERE'\n","print(\"Your computed score:\", score_C)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dYRwjnNoNeBf"},"outputs":[],"source":["score_R = 'FILL IN HERE'\n","print(\"Your computed score:\", score_R)"]},{"cell_type":"markdown","metadata":{"id":"ILj07BjGqAu_"},"source":["### <span style=\"color:green\"><em>end of Problem 3</em></span>"]},{"cell_type":"markdown","metadata":{"id":"ygBr-b6otMVk"},"source":["## Bias and Variance\n","\n","To understand one of the most important concepts in machine learning evaluation, the bias-variance tradeoff, we must first establish what each term means. Simply put, *bias* is the tendency of to systematically over or under-estimate something. For example, if a seesaw has starts off at an incline, then we can say that it is already biased to one side regardless of the weight of the people using it. On the other hand, *variance* measures how far some metric is from a mean value. High variance corresponds to more spread out observations while low variance corresponds to datapoints that're clumped closer together.\n","\n","How do these terms work in machine learning models? One way to think about a model that is highly biased is to consider the worst case- where the model fails to learn anything at all. Then, the model is held to its pre-training parameters, and thus biased towards these results. In the case of variance, the opposite is true. Consider a model whose parameters yield a fairly accurate average result. If it exhibits high variance, then its predictions will vary more from that average result, meaning it is more sensitive to any noise in the data."]},{"cell_type":"markdown","metadata":{"id":"adVUPbRNtMVn"},"source":["## Bias-Variance Tradeoff\n","\n","In the above example, we see the 'bias-variance tradeoff'. Simply put, the bias and variance of a model's predictions must be balanced as much as possible in order to find the best machine learning model for any task. As you may have guessed, high bias inherently means having low variance while high variance means having low bias- hence, the tradeoff."]},{"cell_type":"markdown","metadata":{"id":"XFQc7_TxtMVq"},"source":["## Overfitting and Underfitting\n","\n","Having a high bias means your model did not learn as much as it could have (*underfitting*), while having a high variance means the model was responsive to training data to the point that it does not generalize well (*overfitting*).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F12pAtz0qAvB","outputId":"217762d2-30d6-499c-c16a-23df9f055e14","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697588854746,"user_tz":240,"elapsed":224,"user":{"displayName":"Varun Gande","userId":"11529055052164094479"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Lannister,\n","None,\n","Targaryen,\n","Greyjoy,\n","Night's Watch,\n","Arryn,\n","Stark,\n","Tyrell,\n","Baratheon,\n","Martell,\n","Wildling,\n","Tully\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-2-73c3447839a9>:6: FutureWarning: The default value of regex will change from True to False in a future version.\n","  deaths[\"Allegiances\"] = deaths[\"Allegiances\"].str.replace(pat=r'House (?P<one>.*)', repl=lambda m: m.group('one'))\n"]}],"source":["deaths = pd.read_csv('lecture5dataB (1).csv')\n","deaths['Book of Death'].fillna(0,inplace=True)\n","deaths['Death Year'].fillna(deaths['Death Year'].mean(),inplace=True)\n","deaths.dropna(subset=['Book Intro Chapter'],inplace=True)\n","deaths['Death Chapter'].fillna(deaths['Death Chapter'].mean(),inplace=True)\n","deaths[\"Allegiances\"] = deaths[\"Allegiances\"].str.replace(pat=r'House (?P<one>.*)', repl=lambda m: m.group('one'))\n","\n","print(\",\\n\".join(deaths[\"Allegiances\"].unique()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9b0HejanqAvE","outputId":"1cd4997c-c154-43b1-c5fd-d52f4beb2f24"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Name</th>\n","      <th>Allegiances</th>\n","      <th>Death Year</th>\n","      <th>Book of Death</th>\n","      <th>Death Chapter</th>\n","      <th>Book Intro Chapter</th>\n","      <th>Gender</th>\n","      <th>Nobility</th>\n","      <th>GoT</th>\n","      <th>CoK</th>\n","      <th>SoS</th>\n","      <th>FfC</th>\n","      <th>DwD</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Addam Marbrand</td>\n","      <td>Lannister</td>\n","      <td>299.157377</td>\n","      <td>0.0</td>\n","      <td>40.25</td>\n","      <td>56.0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Aegon Frey (Jinglebell)</td>\n","      <td>None</td>\n","      <td>299.000000</td>\n","      <td>3.0</td>\n","      <td>51.00</td>\n","      <td>49.0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Aegon Targaryen</td>\n","      <td>Targaryen</td>\n","      <td>299.157377</td>\n","      <td>0.0</td>\n","      <td>40.25</td>\n","      <td>5.0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Adrack Humble</td>\n","      <td>Greyjoy</td>\n","      <td>300.000000</td>\n","      <td>5.0</td>\n","      <td>20.00</td>\n","      <td>20.0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Aemon Targaryen (son of Maekar I)</td>\n","      <td>Night's Watch</td>\n","      <td>300.000000</td>\n","      <td>4.0</td>\n","      <td>35.00</td>\n","      <td>21.0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                Name    Allegiances  Death Year  \\\n","0                     Addam Marbrand      Lannister  299.157377   \n","1            Aegon Frey (Jinglebell)           None  299.000000   \n","2                    Aegon Targaryen      Targaryen  299.157377   \n","3                      Adrack Humble        Greyjoy  300.000000   \n","6  Aemon Targaryen (son of Maekar I)  Night's Watch  300.000000   \n","\n","   Book of Death  Death Chapter  Book Intro Chapter  Gender  Nobility  GoT  \\\n","0            0.0          40.25                56.0       1         1    1   \n","1            3.0          51.00                49.0       1         1    0   \n","2            0.0          40.25                 5.0       1         1    0   \n","3            5.0          20.00                20.0       1         1    0   \n","6            4.0          35.00                21.0       1         1    1   \n","\n","   CoK  SoS  FfC  DwD  \n","0    1    1    1    0  \n","1    0    1    0    0  \n","2    0    0    0    1  \n","3    0    0    0    1  \n","6    0    1    1    0  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["deaths.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kAXNzorAqAvG","scrolled":true},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","\n","X = deaths[['Death Year','Book Intro Chapter','Book of Death','Death Chapter']]\n","Y = deaths['Allegiances']\n","x_tr, x_te, y_tr, y_te = train_test_split(X, Y, test_size = 0.2, random_state=42)\n","train_scores = []\n","test_scores = []\n","\n","max_depths = list(range(10,100))\n","for i in max_depths:\n","    model = DecisionTreeClassifier(max_depth=i)\n","\n","    model.fit(x_tr, y_tr)\n","\n","    train_scores.append(model.score(x_tr, y_tr))\n","    test_scores.append(model.score(x_te, y_te))\n","\n","plt.subplots(figsize=(15,5))\n","plt.subplots_adjust(wspace=0.4)\n","plt.subplot(131)\n","plt.plot(max_depths, train_scores)\n","plt.title('Training Score: More complex is better')\n","plt.xlabel('Model Complexity')\n","plt.ylabel('Training Score')\n","plt.subplot(132)\n","plt.plot(max_depths, test_scores)\n","plt.title(\"Testing Score: There's a sweetspot\")\n","plt.xlabel('Model Complexity')\n","plt.ylabel('Testing Score')\n","plt.subplot(133)\n","plt.plot(max_depths, np.subtract(train_scores,test_scores))\n","plt.title(\"Generalization Error\")\n","plt.xlabel('Model Complexity')\n","plt.ylabel('(Training Score) - (Testing Score)')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"4WlCOWCXqAvH"},"source":["## <span style=\"color:green\"><em>Optional Problem</em></span>\n","### Part a\n","Modify the loop above to programmatically find the best `max_depth` for a Decision Tree. Print out the training and testing score of just a model that uses that `max_depth`.\n","You could also try using sklearn's `GridSearchCV` instead, which we will cover in a later lecture.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6cteIx6-qAvI"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"MnYyymAOqAvK"},"source":["### Part b\n","Now, imagine if you get a `Lannister`'s allegiance wrong, there is a much harsher consequence. To be specific, for any Lannister, the penalty of not predicting that they're a Lannister is 5x the normal penalty. Adjust your scoring mechanism using this new metric (still produce a score normalized by baseline). If you used GridSearchCV above, then see if you can use GridSearchCV's `scoring` parameter.\n","Note: we're dealing with classification here, not regression, so you'll need to use a classification loss function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tb9wc49vqAvK"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"mjjL2kN3qAvM"},"source":["### Part c\n","Now, imagine that you care twice as much about people whose Death Year is greater than or equal to `300`. Adjust your scoring mechanism using this new metric (still produce a score normalized by baseline). sklearn's typical scoring parameter doesn't allow for this, so you can't use GridSearchCV (as far as I know -- I could be wrong)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Ali_1-tqAvM"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"8lNmvNm_qAvO"},"source":["### <span style=\"color:green\"><em>end of Optional Problem</em></span>"]},{"cell_type":"markdown","metadata":{"id":"xM4wvFwQtMWh"},"source":["## Feature-Subset Selection Techniques\n","\n","\n","A dataset will usually have many features, many of which will not be useful at all. The key is to determine which are helpful in improving your model.\n","\n","Use the following block to help decide if a particular feature subset selection is helpful for a linear model built on a dataset of a Hungarian city called Szeged. Feel free to modify it to suit your needs.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BfMYgEertMWs","outputId":"7d0c6889-6e47-4744-f544-0f17caccf770"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Temperature (C)</th>\n","      <th>Apparent Temperature (C)</th>\n","      <th>Humidity</th>\n","      <th>Wind Speed (km/h)</th>\n","      <th>Wind Bearing (degrees)</th>\n","      <th>Visibility (km)</th>\n","      <th>Pressure (millibars)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>95936.000000</td>\n","      <td>95936.000000</td>\n","      <td>95936.000000</td>\n","      <td>95936.000000</td>\n","      <td>95936.000000</td>\n","      <td>95936.000000</td>\n","      <td>95936.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>11.940976</td>\n","      <td>10.862531</td>\n","      <td>0.734841</td>\n","      <td>10.804936</td>\n","      <td>187.518773</td>\n","      <td>10.362402</td>\n","      <td>1003.150038</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>9.570671</td>\n","      <td>10.717812</td>\n","      <td>0.195724</td>\n","      <td>6.920727</td>\n","      <td>107.385351</td>\n","      <td>4.173780</td>\n","      <td>117.276976</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>-21.822222</td>\n","      <td>-27.716667</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>4.604167</td>\n","      <td>2.276389</td>\n","      <td>0.600000</td>\n","      <td>5.796000</td>\n","      <td>116.000000</td>\n","      <td>8.372000</td>\n","      <td>1011.890000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>12.033333</td>\n","      <td>12.033333</td>\n","      <td>0.780000</td>\n","      <td>9.933700</td>\n","      <td>180.000000</td>\n","      <td>10.046400</td>\n","      <td>1016.420000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>18.844444</td>\n","      <td>18.844444</td>\n","      <td>0.890000</td>\n","      <td>14.135800</td>\n","      <td>290.000000</td>\n","      <td>14.812000</td>\n","      <td>1021.050000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>39.905556</td>\n","      <td>39.344444</td>\n","      <td>1.000000</td>\n","      <td>63.852600</td>\n","      <td>359.000000</td>\n","      <td>16.100000</td>\n","      <td>1046.380000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Temperature (C)  Apparent Temperature (C)      Humidity  \\\n","count     95936.000000              95936.000000  95936.000000   \n","mean         11.940976                 10.862531      0.734841   \n","std           9.570671                 10.717812      0.195724   \n","min         -21.822222                -27.716667      0.000000   \n","25%           4.604167                  2.276389      0.600000   \n","50%          12.033333                 12.033333      0.780000   \n","75%          18.844444                 18.844444      0.890000   \n","max          39.905556                 39.344444      1.000000   \n","\n","       Wind Speed (km/h)  Wind Bearing (degrees)  Visibility (km)  \\\n","count       95936.000000            95936.000000     95936.000000   \n","mean           10.804936              187.518773        10.362402   \n","std             6.920727              107.385351         4.173780   \n","min             0.000000                0.000000         0.000000   \n","25%             5.796000              116.000000         8.372000   \n","50%             9.933700              180.000000        10.046400   \n","75%            14.135800              290.000000        14.812000   \n","max            63.852600              359.000000        16.100000   \n","\n","       Pressure (millibars)  \n","count          95936.000000  \n","mean            1003.150038  \n","std              117.276976  \n","min                0.000000  \n","25%             1011.890000  \n","50%             1016.420000  \n","75%             1021.050000  \n","max             1046.380000  "]},"execution_count":2,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["weather[noncategorical].describe()"]},{"cell_type":"markdown","metadata":{"id":"FGOJPEm7tMW4"},"source":["# Problem 4 (3 points)\n","\n","Using what you have learned, create a correlation matrix of the data. Use it to decide the three best features to use in predicting Humidity and store those in a list named `three_correlated_features`. Then store the two best features to in a list named `two_correlated_features`. Compare the result of using `three_correlated_features` vs `two_correlated_features` to train a Linear Regression. (When we say compare the results, we mean compare print out the scores)."]},{"cell_type":"markdown","metadata":{"id":"UwN1_pEUtMXD"},"source":["Your results should show you an important lesson about feature selection- you don't always need to have all features to show almost the same results, and selecting a feature subset of lesser size may be more resource-efficient."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"csjZXn9fqAvX"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"4fAVKAfEqAvf"},"source":["### <span style=\"color:green\"><em> end of Problem 4 </em></span>"]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["ChDh8hwjtMVh","EvLepXLMqAut","ht-PLaIa7-td","rRppePJSqAu3","XFQc7_TxtMVq","4WlCOWCXqAvH","xM4wvFwQtMWh","FGOJPEm7tMW4"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"notebookId":"^E||G=G=bpDp\\gp"},"nbformat":4,"nbformat_minor":0}