{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Assignment 5: Model Loss & Accuracy"]},{"cell_type":"markdown","metadata":{"id":"C6D3rHhQqAum"},"source":["### <b>NetID</b>: PLEASE FILL ME IN"]},{"cell_type":"markdown","metadata":{"id":"nuBRyHO14g_W"},"source":["### Problems\n","- Problem 1 (2 points)\n","- Problem 2 (1 point)\n","- Problem 3 (4 points)\n","- Problem 4 (3 points)"]},{"cell_type":"markdown","metadata":{"id":"ChDh8hwjtMVh"},"source":["# Assessing Model Accuracy\n","In this lesson, we look over different ways to evaluate whether machine learning models you have created successfully accomplish their intended objective."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"executionInfo":{"elapsed":1775,"status":"error","timestamp":1697588724291,"user":{"displayName":"Varun Gande","userId":"11529055052164094479"},"user_tz":240},"id":"H870tIe_tMWj","outputId":"8d361bc2-8d5d-40a4-f5e8-91f11eb89b7d","scrolled":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","from matplotlib import pyplot as plt\n","\n","weather = pd.read_csv('lecture5dataA.csv').dropna()\n","noncategorical = [weather.columns[i] for i in range (3,11) if i != 9]\n","print (\"Noncategorical Features: \", noncategorical)\n","weather.head()"]},{"cell_type":"markdown","metadata":{"id":"JWpTFF27tMXD"},"source":["## Loss Functions and Accuracy\n","\n","In evaluating your models, it's important to remember that different models must be evaluated with the appropriate metric. Classification accuracy is not, for example, the same thing as the mean-squared error used in regression problems. Furthermore, a high score in either of those metrics does not prove a model is \"good\"."]},{"cell_type":"markdown","metadata":{"id":"EvLepXLMqAut"},"source":["# Problem 1 (2 points)\n","Edit the lines marked with TODO's below to do the following:\n","1. Create two columns to `temperatures` to store Temperature and Apparent Temperature in Rankines. Rankines is a  weird unit of temperature. Temperature in Rankines is 9/5 * (temperature in Celcius) + 491.67.\n","2. Train and predict two models: one for celcius and one for rankines\n","3. Compare the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uJrP_FSTqAuu"},"outputs":[],"source":["temperatures = weather.loc[:,[\"Temperature (C)\", \"Apparent Temperature (C)\"]]\n","temperatures[\"Temperature (R)\"] = 0 #TODO: replace the 0 (hint: you don't need a loop)\n","temperatures[\"Apparent Temperature (R)\"] = 0 #TODO: replace the 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KtyH-s_FqAuw"},"outputs":[],"source":["celcius_model = LinearRegression()\n","# TODO split data. goal is temperature in celcius, feature is apparent temperature in celcius.\n","# Make sure that the names of your data for the two models are different! Otherwise, one will overwrite the other\n","# Then, fit the model.\n","\n","\n","x_tr_C, x_te_C, y_tr_C, y_te_C = # Fill in here\n","\n","\n","rankines_model = LinearRegression()\n","# TODO same as above, but for rankines\n","\n","\n","x_tr_R, x_te_R, y_tr_R, y_te_R = # Fill in here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGyhDcXBqAuy"},"outputs":[],"source":["from sklearn.metrics import root_mean_squared_error\n","\n","# TODO store the predictions for the test sets\n","celcius_predictions = \"Fill in here\"\n","rankines_predictions = \"Fill in here\"\n","\n","# TODO find mean squared error of each model's predictions\n","celcius_MSE = root_mean_squared_error(\"Fill in here\", \"Fill in here\", squared=True) # TODO\n","rankines_MSE = root_mean_squared_error(\"Fill in here\", \"Fill in here\", squared=True) # TODO\n","\n","print(\"celcius MSE:\", celcius_MSE)\n","print(\"rankines MSE:\", rankines_MSE)\n","print(\"\\n(if the MSE for rankines is 0, you missed something two cells above)\")"]},{"cell_type":"markdown","metadata":{"id":"mlIL6hKoqAuz"},"source":["#### The MSE's of the two models are significantly different -- one is more than triple the other. To inspect this difference, let's plot the predictions of the two models and compare."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aO0zdvI1qAu0"},"outputs":[],"source":["plt.subplots(figsize=(15, 5))\n","plt.subplot(121)\n","plt.scatter(x_te_C, y_te_C)\n","plt.plot(x_te_C, celcius_predictions, 'k', linewidth=4)\n","plt.legend([\"Predictions\",\"Actual Values\"])\n","plt.title('Celcius Linear Regression')\n","plt.xlabel('Apparent Temperature (C)')\n","plt.ylabel('Temperature (C)')\n","\n","plt.subplot(122)\n","plt.scatter(x_te_R, y_te_R)\n","plt.plot(x_te_R, rankines_predictions, 'k', linewidth=4)\n","plt.legend([\"Predictions\",\"Actual Values\"])\n","plt.title('Rankines Linear Regression')\n","plt.xlabel('Apparent Temperature (R)')\n","plt.ylabel('Temperature (R)')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"akGRKGNsqAu2"},"source":["#### The plots look the same! The only significant difference is the scale of the axes. That's why the MSE for Rankines is bigger: Rankines are generally greater than Celcius, and so their error is naturally bigger. To take care of this, we use a _baseline_."]},{"cell_type":"markdown","metadata":{"id":"JcSBtmN3qAu2"},"source":["### <span style=\"color:green\"><em>end of Problem 1</em></span>"]},{"cell_type":"markdown","metadata":{"id":"ht-PLaIa7-td"},"source":["# Problem 2 (1 point)\n","Why do the plots look the same? How do the axes compare? Why is this what we expect?"]},{"cell_type":"markdown","metadata":{"id":"EFdA4Mvo8LNI"},"source":["fill in here"]},{"cell_type":"markdown","metadata":{"id":"rRppePJSqAu3"},"source":["# Problem 3 (4 points)\n","\n","Compute the 'score' of the celcius model using sklearn's .score() method on *celcius_model*. Do the same for the Rankines model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3xIfk86AqAu3"},"outputs":[],"source":["print(\"sklearn's score for Celcius:\", celcius_model.score('FILL IN HERE','FILL IN HERE'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3EAL5ZmtqAu5"},"outputs":[],"source":["print(\"sklearn's score for Rankines:\", rankines_model.score('FILL IN HERE','FILL IN HERE'))"]},{"cell_type":"markdown","metadata":{"id":"vbkGaKLhNeBd"},"source":["But what exactly is .score() doing?\n","\n","When building a model, we typically have a baseline model to compare against. This allows us to see whether or not our model is better than a relatively simple, naive model.\n","\n","In our case, the most simple, naive (baseline) model we can build to predict a location's temperature in the test set is to simply predict the mean of all temperatures across the testing set (for every single test point in the test set).\n","\n","Go ahead and compute the MSE for the outputs of this baseline model: i.e. compute the MSE between the true outputs (i.e. *y_te_C*) and the predicted outputs of this baseline model (i.e. mean of the testing labels, *y_te_C*). Follow the same procedure for the Rankines model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOXPrezmqAu7"},"outputs":[],"source":["test_goal_mean_C = 'FILL IN HERE'\n","baseline_C = np.full((len(celcius_predictions),), test_goal_mean_C)\n","baseline_C_MSE = root_mean_squared_error('FILL IN HERE', 'FILL IN HERE', squared=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xEaLf1qqqAu-"},"outputs":[],"source":["test_goal_mean_R = 'FILL IN HERE'\n","baseline_R = np.full((len(rankines_predictions),), test_goal_mean_R)\n","baseline_R_MSE = root_mean_squared_error('FILL IN HERE', 'FILL IN HERE', squared=True)"]},{"cell_type":"markdown","metadata":{"id":"SflEPrARNeBe"},"source":["Now, compute the normalized score (relative to a baseline model) defined as: norm_score = 1 - model_MSE / baseline_MSE. If you did everything correctly, your computed normalized scores should be exactly the same as sklearn's .score() method.\n","\n","If necessary, ask TAs for help!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L0IAG2z2NeBf"},"outputs":[],"source":["score_C = 'FILL IN HERE'\n","print(\"Your computed score:\", score_C)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dYRwjnNoNeBf"},"outputs":[],"source":["score_R = 'FILL IN HERE'\n","print(\"Your computed score:\", score_R)"]},{"cell_type":"markdown","metadata":{"id":"ILj07BjGqAu_"},"source":["### <span style=\"color:green\"><em>end of Problem 3</em></span>"]},{"cell_type":"markdown","metadata":{"id":"ygBr-b6otMVk"},"source":["## Bias and Variance\n","\n","To understand one of the most important concepts in machine learning evaluation, the bias-variance tradeoff, we must first establish what each term means. Simply put, *bias* is the tendency of to systematically over or under-estimate something. For example, if a seesaw has starts off at an incline, then we can say that it is already biased to one side regardless of the weight of the people using it. On the other hand, *variance* measures how far some metric is from a mean value. High variance corresponds to more spread out observations while low variance corresponds to datapoints that're clumped closer together.\n","\n","How do these terms work in machine learning models? One way to think about a model that is highly biased is to consider the worst case- where the model fails to learn anything at all. Then, the model is held to its pre-training parameters, and thus biased towards these results. In the case of variance, the opposite is true. Consider a model whose parameters yield a fairly accurate average result. If it exhibits high variance, then its predictions will vary more from that average result, meaning it is more sensitive to any noise in the data."]},{"cell_type":"markdown","metadata":{"id":"adVUPbRNtMVn"},"source":["## Bias-Variance Tradeoff\n","\n","In the above example, we see the 'bias-variance tradeoff'. Simply put, the bias and variance of a model's predictions must be balanced as much as possible in order to find the best machine learning model for any task. As you may have guessed, high bias inherently means having low variance while high variance means having low bias- hence, the tradeoff."]},{"cell_type":"markdown","metadata":{"id":"XFQc7_TxtMVq"},"source":["## Overfitting and Underfitting\n","\n","Having a high bias means your model did not learn as much as it could have (*underfitting*), while having a high variance means the model was responsive to training data to the point that it does not generalize well (*overfitting*).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":224,"status":"ok","timestamp":1697588854746,"user":{"displayName":"Varun Gande","userId":"11529055052164094479"},"user_tz":240},"id":"F12pAtz0qAvB","outputId":"217762d2-30d6-499c-c16a-23df9f055e14"},"outputs":[],"source":["deaths = pd.read_csv('lecture5dataB (1).csv')\n","deaths['Book of Death'].fillna(0,inplace=True)\n","deaths['Death Year'].fillna(deaths['Death Year'].mean(),inplace=True)\n","deaths.dropna(subset=['Book Intro Chapter'],inplace=True)\n","deaths['Death Chapter'].fillna(deaths['Death Chapter'].mean(),inplace=True)\n","deaths[\"Allegiances\"] = deaths[\"Allegiances\"].str.replace(pat=r'House (?P<one>.*)', repl=lambda m: m.group('one'))\n","\n","print(\",\\n\".join(deaths[\"Allegiances\"].unique()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9b0HejanqAvE","outputId":"1cd4997c-c154-43b1-c5fd-d52f4beb2f24"},"outputs":[],"source":["deaths.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kAXNzorAqAvG","scrolled":true},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","\n","X = deaths[['Death Year','Book Intro Chapter','Book of Death','Death Chapter']]\n","Y = deaths['Allegiances']\n","x_tr, x_te, y_tr, y_te = train_test_split(X, Y, test_size = 0.2, random_state=42)\n","train_scores = []\n","test_scores = []\n","\n","max_depths = list(range(10,100))\n","for i in max_depths:\n","    model = DecisionTreeClassifier(max_depth=i)\n","\n","    model.fit(x_tr, y_tr)\n","\n","    train_scores.append(model.score(x_tr, y_tr))\n","    test_scores.append(model.score(x_te, y_te))\n","\n","plt.subplots(figsize=(15,5))\n","plt.subplots_adjust(wspace=0.4)\n","plt.subplot(131)\n","plt.plot(max_depths, train_scores)\n","plt.title('Training Score: More complex is better')\n","plt.xlabel('Model Complexity')\n","plt.ylabel('Training Score')\n","plt.subplot(132)\n","plt.plot(max_depths, test_scores)\n","plt.title(\"Testing Score: There's a sweetspot\")\n","plt.xlabel('Model Complexity')\n","plt.ylabel('Testing Score')\n","plt.subplot(133)\n","plt.plot(max_depths, np.subtract(train_scores,test_scores))\n","plt.title(\"Generalization Error\")\n","plt.xlabel('Model Complexity')\n","plt.ylabel('(Training Score) - (Testing Score)')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"4WlCOWCXqAvH"},"source":["## <span style=\"color:green\"><em>Optional Problem</em></span>\n","### Part a\n","Modify the loop above to programmatically find the best `max_depth` for a Decision Tree. Print out the training and testing score of just a model that uses that `max_depth`.\n","You could also try using sklearn's `GridSearchCV` instead, which we will cover in a later lecture.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6cteIx6-qAvI"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"MnYyymAOqAvK"},"source":["### Part b\n","Now, imagine if you get a `Lannister`'s allegiance wrong, there is a much harsher consequence. To be specific, for any Lannister, the penalty of not predicting that they're a Lannister is 5x the normal penalty. Adjust your scoring mechanism using this new metric (still produce a score normalized by baseline). If you used GridSearchCV above, then see if you can use GridSearchCV's `scoring` parameter.\n","Note: we're dealing with classification here, not regression, so you'll need to use a classification loss function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tb9wc49vqAvK"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"mjjL2kN3qAvM"},"source":["### Part c\n","Now, imagine that you care twice as much about people whose Death Year is greater than or equal to `300`. Adjust your scoring mechanism using this new metric (still produce a score normalized by baseline). sklearn's typical scoring parameter doesn't allow for this, so you can't use GridSearchCV (as far as I know -- I could be wrong)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Ali_1-tqAvM"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"8lNmvNm_qAvO"},"source":["### <span style=\"color:green\"><em>end of Optional Problem</em></span>"]},{"cell_type":"markdown","metadata":{"id":"xM4wvFwQtMWh"},"source":["## Feature-Subset Selection Techniques\n","\n","\n","A dataset will usually have many features, many of which will not be useful at all. The key is to determine which are helpful in improving your model.\n","\n","Use the following block to help decide if a particular feature subset selection is helpful for a linear model built on a dataset of a Hungarian city called Szeged. Feel free to modify it to suit your needs.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BfMYgEertMWs","outputId":"7d0c6889-6e47-4744-f544-0f17caccf770"},"outputs":[],"source":["weather[noncategorical].describe()"]},{"cell_type":"markdown","metadata":{"id":"FGOJPEm7tMW4"},"source":["# Problem 4 (3 points)\n","\n","Using what you have learned, create a correlation matrix of the data. Use it to decide the three best features to use in predicting Humidity and store those in a list named `three_correlated_features`. Then store the two best features to in a list named `two_correlated_features`. Compare the result of using `three_correlated_features` vs `two_correlated_features` to train a Linear Regression. (When we say compare the results, we mean compare print out the scores)."]},{"cell_type":"markdown","metadata":{"id":"UwN1_pEUtMXD"},"source":["Your results should show you an important lesson about feature selection- you don't always need to have all features to show almost the same results, and selecting a feature subset of lesser size may be more resource-efficient."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"csjZXn9fqAvX"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"4fAVKAfEqAvf"},"source":["### <span style=\"color:green\"><em> end of Problem 4 </em></span>"]}],"metadata":{"colab":{"collapsed_sections":["ChDh8hwjtMVh","EvLepXLMqAut","ht-PLaIa7-td","rRppePJSqAu3","XFQc7_TxtMVq","4WlCOWCXqAvH","xM4wvFwQtMWh","FGOJPEm7tMW4"],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"notebookId":"^E||G=G=bpDp\\gp"},"nbformat":4,"nbformat_minor":0}
