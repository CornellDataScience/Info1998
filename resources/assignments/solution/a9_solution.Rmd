# Unsupervised Learning

## NETID: SOLUTION


Previously, we have been focusing on supervised learning. Recall that we covered two types of supervised learning models: regression for predicting continuous variables and classification for predicting categorical ones.

For instance, for classification problems like a spam detector, you would feed the model training data with some data labeled as 'spam', and others as 'ham' (not spam). On the other hand, for regression problems like predicting housing prices, you would instead pass in some housing data including the associated house price. In both cases, the training data you feed into your model includes the desired solutions (i.e. spam/ham, house price). 

However, what if we are interested in something that isn't included as a target variable in the dataset? Then we need to rely on *unsupervised learning* instead to gain insight into characteristics of the data that aren't explicity identified. 

Note that there are many different types of unsupervised learning algorithms, each with different use cases. For example:
 - Clustering
     - Hierarchical Cluster Analysis (HCA)
     - k-Means
     - Gaussian Mixture Models (GMMs)
     - Expected Maximization
 - Visualization and Dimensionality Reduction
     - Principal Component Analysis (PCA)
     - Kernel PCA
     - Locally Linear Embedding (LLE)
     - t-Distibuted Stochastic Linear Embedding (t-SNE)
 - Association Rule Learning
     - Apriori
     - Eclat

However, in this class, we shall only cover three of the clustering algorithms listed above: Hierarchical Clustering, k-Means Clustering, and Gaussian Mixture Models. Let's get started.

(And as usual, if you are curious about anything data science that we mentioned, or that you've heard of outside of class, feel free to come talk to us about it after class or during office hours!)


## Hierarchical Clustering

Hierarchical clustering groups observations into multiple levels of sets; the top-level set includes all of the data, and the bottom-level sets contain individual observations. The levels in between contain sets of observations with similar features. Let's take a look at a dataset.

```{r}
library(datasets)
data(iris)
df = iris[,-5]
head(df)
```

The iris dataset contains observations on three iris species. Features include sepal length, sepal width, petal length, and petal width. Notice how the species is not identified in the version of the dataset that we use above. Let's see if we can gain this information through hierarchical clustering. 


### Problem 1

Perform hierarchical clustering on the iris dataset and plot the resulting clusters (as a dendrogram). How many clusters do you see? How does this compare to the number of clusters you expect to see?

```{r}
df = scale(df) # need to scale the data
distance_matrix = dist(df)
clusters = hclust(distance_matrix)
plot(clusters)
```

#### A side note about distances:
To find the "similarity" for a group of clusters, we simply measure the magnitude of the vector between two the two data points (in whatever dimension we are working with). There are many ways to calculate the magnitude of this vector: for instance, in class we gave the example of either using the euclidean distance or the manhattan distance.



## K-Means Clustering

K-means clustering is another technique for finding groupings in a dataset. Recall kNN from a few lectures ago; k-means uses a similar geometric approach. The k-means algorithm takes in the number of clusters (k), computes the centroid of each cluster, and assigns observations to clusters based on distance from the centroid. Let's take another look at the iris dataset.


```{r}
plot(iris$Petal.Length, iris$Petal.Width, col = iris$Species, pch = 16, xlab = "Petal Length", ylab = "Petal width", main = "Petal Width vs. Length colored by Species")
```
We can clearly see the three groupings in the data corresponding to the three species. Let's see if we can replicate these clusters using the k-means algorithm. 



### Problem 2a

Perform k-means clustering on the iris dataset (using only the petal length and petal width features).

```{r}
df_2 = df[,-c(1,2)]

kcluster = kmeans(df_2,centers = 3)
kcluster
```

```{r}
#plotting our results
library(factoextra) #might need to install this package if you have not used it prior
fviz_cluster(kcluster, data = df_2, stand = FALSE)
```

## Problem 2b

Now we will also apply kmeans clustering on a different dataset about wines. In the section of code specified below, create a kmeans model stored in a variable named `kmeans` and train it on the wines dataset. Use the number of unique values in the 'target' column as the number of clusters.

```{r}
df = read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv", sep = ";")
head(df)
```
```{r}
#making new class column to use to predict (just binning quality)
df$class = 0

for(val in 1:length(df[,1])){
  if(df[val, 'quality']<=5){
    df[val,'class']= 1
  }else if (df[val, 'quality']<7){
    df[val,'class'] = 2
  }else{
    df[val,'class'] = 3
  }
}

df_without_class = df[,-13]
```

```{r}
k = length(unique(df$class))
X = df_without_class[,c(5,11)]

kcluster = kmeans(X,centers = k)
kcluster
```

```{r}
plot(df$chlorides, df$alcohol, col =  c("red", "green","blue")[df$class], pch = 16)
fviz_cluster(kcluster, data = X, stand = FALSE)
```

## GMMS

The k-means algorithm we just covered have a major flaw - they do not account for variance. You perhaps can recall from your stats class that the variance, visually, is related to the width of a bell shaped curve. Extending on this visual, you can imagine the variance in two-dimensions to just be the radius of a sphere.

Whenever we perform k-means clustering on a model, it groups the data into clusters of circles (in 2 dimension, and hyper-spheres in higher dimensions). This works fine if your data is circular, but performs badly when it is in any shape otherwise. Here is a good visual example: https://towardsdatascience.com/gaussian-mixture-models-d13a5e915c8e.

In such cases, we turn to GMMs. GMMs also have an added advantage of not giving hard assignments into clusters (which k-means does), rather giving 'soft assignments', where each data point can be generated by any of the distributions with corresponding probability.

This concept may seem rather abstract right now - let's see it visually!

```{r}
#installing package helpful with creating the blobs
install.packages("remotes")
remotes::install_github("elbamos/clusteringdatasets")
```

Run these 4 cells, comparing the accuracy of the clustering when the blobs are of different shapes (elongated vs. more circular)

```{r}
library(clusteringdatasets)
set.seed(30)
blobs = make_blobs(n_samples = 400, n_features = 2, centers = 4, cluster_std = 0.60)

kcluster = kmeans(blobs$samples,centers = 4)

plot(blobs$samples[,1], blobs$samples[,2], col = blobs$labels)
```
```{r}
#plotting our results
library(factoextra) #might need to install this package if you have not used it prior
inp = as.data.frame(blobs$samples)
fviz_cluster(kcluster, data = inp, stand = FALSE)
```
```{r}
library(clusteringdatasets)
set.seed(1)
blobs = make_blobs(n_samples = 400, n_features = 2, centers = 4, cluster_std = 0.60)

kcluster = kmeans(blobs$samples,centers = 4)

plot(blobs$samples[,1], blobs$samples[,2], col = blobs$labels)
```

```{r}
#plotting our results
inp = as.data.frame(blobs$samples)
fviz_cluster(kcluster, data = inp, stand = FALSE)
```
As you can see, a K-Means model generally performs quite poorly when fitting any non-circular shapes. Let's see how a GMM performs instead:
  

## Problem 3 (optional)

In the cell below, you can see code for how to train a GMM and plot the points while coloring them based on the prediction of the GMM. Run the two cells and compare the output of the GMM classification to the KMeans done above. Does GMM preform better on the non-circular shapes?


```{r}
library(mclust)
```

```{r}
set.seed(1)
blobs = make_blobs(n_samples = 400, n_features = 2, centers = 4, cluster_std = 0.60)
gmm.mclust = Mclust(blobs$samples)
plot(gmm.mclust, what=c("classification"))
```

```{r}
set.seed(30)
blobs = make_blobs(n_samples = 400, n_features = 2, centers = 4, cluster_std = 0.60)
gmm.mclust = Mclust(blobs$samples)
plot(gmm.mclust, what=c("classification"))
```

PCA (optional)


We use principal component analysis to determine which features contribute most significantly to variation in a dataset. Let's do an example on HR analytics data. If you're interested, you can read about the dataset here: https://www.kaggle.com/giripujar/hr-analytics.

```{r}
df = read.csv('lecture9data.csv')
head(df)
```
```{r}
df = df[,-c(7,9,10)] #dropping left, Department and Salary (categorical)
head(df)
```

```{r}
library(factoextra)

df.pca = prcomp(df, scale = T) #performing PCA
fviz_eig(df.pca) #plot fraction of variance explained by each component
```


Now we need to determine the relationship between components and features. It's important to understand that a component does not correspond to a single feature; a component may be related to multiple features.


```{r}
t(df.pca$rotation)
```

Each number is the correlation between the feature and the component. For example, we can see that the feature which is most strongly correlated with component 2 is satisfaction_level.


## Question 4 (Advanced - Optional)
Based on the results of the principal component analysis above, which features would you expect are most important to include in a predictive model? Why?


### Further Readings:

Actually, there are four major categories of machine learning systems: supervised learning, unsupervised learning, semisupervised learning, and reinforcement learning. Semisupervised learning, as you can guess, involved models that take in __partially labeled data__ (usually a lot of unlabeled data and just a little labeled data), while reinforcement learning covers goal-oriented algorithms that repeatedly learn and master a particular (usuallly complex task). Reinforcement learning is often the basis of many "AI learns to play..." youtube and facebook videos.

*Side note, you can watch people use reinforcement learning to beat games*  
- https://www.youtube.com/watch?v=L4KBBAwF_bE  (super cool)
- https://www.youtube.com/watch?v=MasxAN-xZIU (a less successful - but no less cool - attempt)

To get an overview of these different categories, we highly recommend you read the very first chapter of this book: Hands-On Machine Learning with Scikit-Learn & Tensorflow: http://index-of.es/Varios-2/Hands%20on%20Machine%20Learning%20with%20Scikit%20Learn%20and%20Tensorflow.pdf (and continue reading if you want to learn how to implement them in Python). In fact, if you go through the book, some of the concepts should seem familiar to you, as we covered them either briefly or somewhat in depth throughout the semester. Indeed, be proud of yourself: you already have the basic concepts of machine learning and data science more or less nailed down!