{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"notebookId":"^EG=G=bpDp\\gp","colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"vfHuUYUcyiV9"},"source":["# NETID: <fill in here\\>"]},{"cell_type":"markdown","source":["### Problems\n","- Problem 1 (3 points total)\n","  - 1a (2 points)\n","  - 1b (1 point)\n","- Problem 2 (6 points total)\n","  - 2a (2 points)\n","  - 2b (2 points)\n","  - 2c (2 points)\n","- Problem 3 (1 point)\n","- Bonus (2 points)"],"metadata":{"id":"nbbZkwsabYBd"}},{"cell_type":"markdown","metadata":{"id":"7EwRP4d5yiWA"},"source":["# Applications of Supervised Learning"]},{"cell_type":"markdown","metadata":{"id":"YbZzB_tTyiWB"},"source":["Last class we covered a popular machine learning model used for classification: K-Nearest Neighbors (KNN). In this lecture, we went over two more classification models: Decision Trees and Logistic Regression. Like KNN, each of these models have their own underlying assumptions and advantages."]},{"cell_type":"markdown","source":["### Import necessary packages"],"metadata":{"id":"PfUL_l4_q9Ua"}},{"cell_type":"code","metadata":{"id":"FjL8712iyiWG","executionInfo":{"status":"ok","timestamp":1698681939470,"user_tz":240,"elapsed":2299,"user":{"displayName":"Deniz Boloni-Turgut","userId":"05746704740663907513"}}},"source":["import pandas as pd\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn import tree\n","from sklearn import datasets\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.linear_model import LogisticRegression"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ADXaco3fyiWC"},"source":["# Decision Trees"]},{"cell_type":"markdown","metadata":{"id":"ZDXmfq7KyiWD"},"source":["The decision tree algorithm can be used to do both classification as well as regression. It has the advantage of not assuming a linear model*. Decisions trees are usually easy to represent visually which makes it easy to understand how the model actually works.\n","\n","\\* Decision trees are piecewise linear - they have linear boundaries for components, but having multiple branches/layers makes non-linear overall."]},{"cell_type":"markdown","source":["### Geometric Intuition\n","\n","In the bottom photo, we can interpret decision trees abstractly as tree figures. The tree branches from a deciding condition (node) into another deciding condition or a final classification. For instance, at the top node, a sample can either have \\<61k income or \\>=61k income. If it is the former, we classify the sample as not having attended the Burning Man festival. Otherwise, we continue branching.\n","\n","These condition nodes can be visualized as linear boundaries on a coordinate system. Notice which conditions correspond to which lines. Also note how some boundaries do not stretch the entire plane, because they were branched off other nodes."],"metadata":{"id":"IAcN-VpfkgP_"}},{"cell_type":"markdown","metadata":{"id":"n7MCq3xFyiWD"},"source":["![image](https://docs.microsoft.com/en-us/azure/machine-learning/studio/media/algorithm-choice/image5.png)"]},{"cell_type":"markdown","metadata":{"id":"9ZlPObQYyiWE"},"source":["### Mathematical Intuition\n","To understand the mathematical basis behind decision trees, we ask a motivation question: How do we know what conditions to choose to split upon?\n","\n","We can create a measure to determine the quality of a condition node. This measure can be based on what feature we choose, as well as the specific point we decide to split upon. Data scientists refer to this quantity as *entropy*.\n","\n","The goal of the full decision tree algorithm is to take the necessary steps to minimize entropy, choosing the right features at every stage to accomplish this."]},{"cell_type":"markdown","metadata":{"id":"XuC97nzAyiWI"},"source":["### Example: Breast Cancer Diagnosis\n","The following dataset contains information about digitized images of a fine needle aspirate (FNA) of a breast mass. Each row in our dataset contains data for a patient. The 'diagnosis' column tells us the outcomne of whether or not a patient was diagnosis was benign (b) or malignant (m)."]},{"cell_type":"code","metadata":{"id":"GsV5x5fDyiWJ","colab":{"base_uri":"https://localhost:8080/","height":381},"executionInfo":{"status":"error","timestamp":1698681944514,"user_tz":240,"elapsed":403,"user":{"displayName":"Deniz Boloni-Turgut","userId":"05746704740663907513"}},"outputId":"6bd6ab3c-cc27-4b24-c209-43dc7fc5b4fa"},"source":["df = pd.read_csv('lecture7example.csv')\n","X=df.drop(['id', 'diagnosis', 'Unnamed: 32'], axis=1)\n","Y=df['diagnosis']\n","df.head()"],"execution_count":2,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-6e6a91178daa>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lecture7example.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'diagnosis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Unnamed: 32'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'diagnosis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lecture7example.csv'"]}]},{"cell_type":"code","metadata":{"id":"AuoTL-jYyiWK"},"source":["X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=1998)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oj8ExcbAyiWK"},"source":["Last week, we built a KNN classifier or this problem. In the code below we created a test-train split of our data and trained a KNN classifer. As we learned last class, accuracy_score() calculates the ratio of correct prediction we make."]},{"cell_type":"code","metadata":{"id":"-5y68C5PyiWL"},"source":["# K-nearest neighbors\n","knn = KNeighborsClassifier()\n","knn.fit(X_train, Y_train)\n","knn_pred_train = knn.predict(X_train)\n","knn_pred_test = knn.predict(X_test)\n","print(\"Train Accuracy: \", accuracy_score(Y_train, knn_pred_train))\n","print(\"Test Accuracy: \", accuracy_score(Y_test, knn_pred_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5rkbMsOlyiWM"},"source":["## Problem 1a (2 pts)\n","Our knn-classifier performed pretty well at predicting which cases are malignant and wich are benign. Now we are going to see how a decision tree peforms. In the next cell, train the decision tree classifier on our training data, and then calulate the training accuracy and testing accuracy."]},{"cell_type":"code","metadata":{"id":"o9nEt05NyiWN"},"source":["# This is the function we use to create the decision tree model\n","model = tree.DecisionTreeClassifier(max_depth = 5)\n","\n","# TODO: train the model\n","# FILL IN HERE\n","\n","# TODO: Calculate the training and testing accuracy\n","dtree_pred_train = \"FILL IN HERE\"\n","dtree_pred_test = \"FILL IN HERE\"\n","print(\"Train Accuracy: \", \"FILL IN HERE\")\n","print(\"Test Accuracy: \", \"FILL IN HERE\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vWObiIwmyiWN"},"source":["## Problem 1b (1 pt)\n","Interpret the accuracy values you found to with the DecisionTreeClassifier with. Please make sure to answer the following questions:\n","1. How do these scores differ with the scores of the KNN classifier?\n","2. Is the model underfitting or overfitiing our data?\n","3. How do the scores change as we vary the max_depth of our tree?"]},{"cell_type":"markdown","metadata":{"id":"JP1RV1H1yiWO"},"source":["Fill in answer here:"]},{"cell_type":"markdown","metadata":{"id":"zkr6mWjQyiWO"},"source":["# Logistic Regression"]},{"cell_type":"markdown","metadata":{"id":"IVD31RFPyiWP"},"source":["Logistic regression, like linear regression, is a generalized linear model. However, the final output of a logistic regression model is not continuous; it is binary (0 or 1). The following sections will explain how this works."]},{"cell_type":"markdown","metadata":{"id":"aOx9CNjMyiWP"},"source":["### A Mathematical Overview (for the brave)\n","The goal of logistic regression is to take a set of datapoints and classify them. This means that we expect to have discrete outputs representing a set of classes. In simple logistic regression, this must be a binary set: our classes must be one of only two possible values. Here are some things that are sometimes modeled as binary classes:\n","\n","<li> Sick or Not Sick </li>\n","<li> Rainy or Dry </li>\n","<li> Democrat or Republican </li>\n","\n","The objective is to find an equation that is able to take input data and classify it into one of the two classes. Luckily, the logistic equation is for just such a task."]},{"cell_type":"markdown","metadata":{"id":"7hBxR-a3yiWQ"},"source":["The <b>logistic equation</b> is the basis of the logistic regression model. It looks like this:\n","\n","![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/5e648e1dd38ef843d57777cd34c67465bbca694f)\n","\n","The t in the equation is some linear combination of n variables, or a linear function in an n-dimensional feature space. The formulation of t therefore has the form ax+b. In fitting a logistic regression model, the goal is therefore to minimize error in the logistic equation with the chosen t (of the form ax+b)  by tuning a and b.\n","\n","\n","The logistic equation (also known as the sigmoid function) works as follows:\n","1. Takes an input of n variables\n","2. Takes a linear combination of the variables as parameter t (this is another way of saying t has the form ax+b)\n","3. Outputs a value given the input and parameter t\n","\n","The output of the logistic equation is always between 0 and 1.\n","\n","A visualization of the outputs of the logistic equation is as below (note that this is but one possible output of a logit regression model):\n","![image](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)"]},{"cell_type":"markdown","metadata":{"id":"YHv3bxNcyiWQ"},"source":["### Income Prediction\n","We'll use logistic regression to predict whether annual income is greater than $50k based on census data. You can read more about the dataset <a href=\"https://www.kaggle.com/uciml/adult-census-income\">here</a>."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"aVNkVWGtyiWS","colab":{"base_uri":"https://localhost:8080/","height":296},"executionInfo":{"status":"ok","timestamp":1698683046687,"user_tz":240,"elapsed":1010,"user":{"displayName":"Deniz Boloni-Turgut","userId":"05746704740663907513"}},"outputId":"92fd892b-85f4-47ab-d388-9fcb8d37ff3a"},"source":["inc_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data', header = None, names = ['age', 'workclass', 'fnlwgt', 'education', 'education.num', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'capital.gain', 'capital.loss', 'hours.per.week', 'native.country', 'income'])\n","# drop null values\n","inc_data = inc_data.dropna()\n","inc_data.head()\n"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   age          workclass  fnlwgt   education  education.num  \\\n","0   39          State-gov   77516   Bachelors             13   \n","1   50   Self-emp-not-inc   83311   Bachelors             13   \n","2   38            Private  215646     HS-grad              9   \n","3   53            Private  234721        11th              7   \n","4   28            Private  338409   Bachelors             13   \n","\n","        marital.status          occupation    relationship    race      sex  \\\n","0        Never-married        Adm-clerical   Not-in-family   White     Male   \n","1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n","2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n","3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n","4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n","\n","   capital.gain  capital.loss  hours.per.week  native.country  income  \n","0          2174             0              40   United-States   <=50K  \n","1             0             0              13   United-States   <=50K  \n","2             0             0              40   United-States   <=50K  \n","3             0             0              40   United-States   <=50K  \n","4             0             0              40            Cuba   <=50K  "],"text/html":["\n","  <div id=\"df-f4a21df5-9946-40f7-9ad8-968938737e29\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>workclass</th>\n","      <th>fnlwgt</th>\n","      <th>education</th>\n","      <th>education.num</th>\n","      <th>marital.status</th>\n","      <th>occupation</th>\n","      <th>relationship</th>\n","      <th>race</th>\n","      <th>sex</th>\n","      <th>capital.gain</th>\n","      <th>capital.loss</th>\n","      <th>hours.per.week</th>\n","      <th>native.country</th>\n","      <th>income</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>39</td>\n","      <td>State-gov</td>\n","      <td>77516</td>\n","      <td>Bachelors</td>\n","      <td>13</td>\n","      <td>Never-married</td>\n","      <td>Adm-clerical</td>\n","      <td>Not-in-family</td>\n","      <td>White</td>\n","      <td>Male</td>\n","      <td>2174</td>\n","      <td>0</td>\n","      <td>40</td>\n","      <td>United-States</td>\n","      <td>&lt;=50K</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>50</td>\n","      <td>Self-emp-not-inc</td>\n","      <td>83311</td>\n","      <td>Bachelors</td>\n","      <td>13</td>\n","      <td>Married-civ-spouse</td>\n","      <td>Exec-managerial</td>\n","      <td>Husband</td>\n","      <td>White</td>\n","      <td>Male</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>13</td>\n","      <td>United-States</td>\n","      <td>&lt;=50K</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>38</td>\n","      <td>Private</td>\n","      <td>215646</td>\n","      <td>HS-grad</td>\n","      <td>9</td>\n","      <td>Divorced</td>\n","      <td>Handlers-cleaners</td>\n","      <td>Not-in-family</td>\n","      <td>White</td>\n","      <td>Male</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>40</td>\n","      <td>United-States</td>\n","      <td>&lt;=50K</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>53</td>\n","      <td>Private</td>\n","      <td>234721</td>\n","      <td>11th</td>\n","      <td>7</td>\n","      <td>Married-civ-spouse</td>\n","      <td>Handlers-cleaners</td>\n","      <td>Husband</td>\n","      <td>Black</td>\n","      <td>Male</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>40</td>\n","      <td>United-States</td>\n","      <td>&lt;=50K</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>28</td>\n","      <td>Private</td>\n","      <td>338409</td>\n","      <td>Bachelors</td>\n","      <td>13</td>\n","      <td>Married-civ-spouse</td>\n","      <td>Prof-specialty</td>\n","      <td>Wife</td>\n","      <td>Black</td>\n","      <td>Female</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>40</td>\n","      <td>Cuba</td>\n","      <td>&lt;=50K</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f4a21df5-9946-40f7-9ad8-968938737e29')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-f4a21df5-9946-40f7-9ad8-968938737e29 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-f4a21df5-9946-40f7-9ad8-968938737e29');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-ae99649b-d9d7-4019-93f6-ede0308e165b\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ae99649b-d9d7-4019-93f6-ede0308e165b')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-ae99649b-d9d7-4019-93f6-ede0308e165b button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"MtCBEu55yiWT"},"source":["Our goal is to predict whether a person's income is less than <=50K  or >50K. Right now the data in the income column is stored as a string, but we want to look at it as binary data.\n","\n","Below, we have converted the data in that column so that an income value of <=50K would be a 0, and an income value of >50K would be a 1. We iterate over the dataframe and use an if/else statement with \" <=50K\" and \" >50K\" (notice the spaces), but we can alternatively use `pd.get_dummies()`."]},{"cell_type":"code","metadata":{"id":"PpdZzTuUyiWT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8e4adf26-fa1c-4113-a29e-3fd974ea9f82"},"source":["inc = inc_data['income']\n","\n","for i in range(0, len(inc)):\n","    if inc[i] == \" <=50K\":\n","        inc[i] = 0\n","    elif inc[i] == \" >50K\":\n","        inc[i] = 1\n","print(inc)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-34b1a8e402bd>:5: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  inc[i] = 0\n","<ipython-input-5-34b1a8e402bd>:7: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  inc[i] = 1\n"]}]},{"cell_type":"markdown","metadata":{"id":"tjX-1YoJyiWU"},"source":["Instead of manually converting all categorical data to quantitative data, we will use the LabelEncoder function."]},{"cell_type":"code","metadata":{"id":"T6l1g4F8yiWU","colab":{"base_uri":"https://localhost:8080/","height":250},"executionInfo":{"status":"error","timestamp":1698683041221,"user_tz":240,"elapsed":200,"user":{"displayName":"Deniz Boloni-Turgut","userId":"05746704740663907513"}},"outputId":"fd6d47c9-ea09-467a-821a-7d4e67fa1c75"},"source":["# the column is present in both categorical and numeric form\n","del inc_data['education']\n","\n","# convert all features to categorical integer values\n","enc = LabelEncoder()\n","for i in inc_data.columns:\n","    inc_data[i] = enc.fit_transform(inc_data[i])"],"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-aafc855541dd>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# the column is present in both categorical and numeric form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0minc_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'education'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# convert all features to categorical integer values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'inc_data' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"aJ98Jmv8yiWU"},"source":["## Problem 2a (2 pts)\n","\n","Build a logistic regression model predicting income based on other income related factors (e.g. `education.num`). You should split the dataset into a training set and a test set as covered previously in the course, fit the model on the observations in the training set, and predict the target variable for the test set. Save your predictions in a variable named `predictions`."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"hQHYVp69yiWU"},"source":["# we separate X (features) and income Y (target)\n","incX = inc_data[['education.num']]\n","incY = inc_data['income']\n","\n","# TODO: train test split your data with 20% being used for testing\n","incX_train, incX_test, incY_train, incY_test = \"FILL IN HERE\"\n","\n","# This is the function we use to create the logistic regression model (default k=5)\n","model = LogisticRegression()\n","\n","# TODO: fit the model using the train data\n","# FILL IN HERE\n","\n","# TODO store the predictions for the training and test set\n","pred_train = \"FILL IN HERE\"\n","pred_test = \"FILL IN HERE\"\n","\n","print(\"Test Accuracy: \", accuracy_score(\"FILL IN HERE\", \"FILL IN HERE\"))\n","print(\"Training Accuracy: \", accuracy_score(\"FILL IN HERE\", \"FILL IN HERE\"))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"31cMv8yvyiWV"},"source":["\n","\n","## Problem 2b (2 pts):\n","Let's see how a decision tree classifier performs with different `max_depth` values.\n","\n","Complete the following code so we find the `max_depth` that gives us the best test accuracy. We can do this by iterating over various values of depth `k`, training."]},{"cell_type":"code","metadata":{"id":"6zlldW3MyiWV"},"source":["best_depth = 1      # Keep track of depth that produces tree with highest accuracy\n","best_accuracy = 0   # The best accuracy from a given tree\n","for k in range(1, 100):\n","    # TODO: create and fit a model of depth k\n","    model = \"FILL IN HERE\"\n","\n","    # TODO: find the accuracy of the model's predictions (pred_test)\n","    # compared to the actual samples, and score the accuracy in acc_test.\n","    pred_test = \"FILL IN HERE\"\n","    acc_test = \"FILL IN HERE\"\n","\n","    # TODO: compare the accuracy found with the best current depth/accuracy found\n","    # and update if necessary\n","\n","    # YOUR CODE GOES HERE\n","\n","print(best_accuracy)\n","print(best_depth)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xOihji0xyiWV"},"source":["## Problem 2c (2 pts):\n","Using the most accurate depth value found in part (b), estimate the ERROR (not accuracy) of your model by using 5-fold cross validation. Refer the documentation found [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)"]},{"cell_type":"code","metadata":{"id":"tQDKyvqLyiWV"},"source":["from sklearn.model_selection import KFold\n","\n","# Fill in code here\n","kf = \"FILL IN HERE\"\n","errors = 0\n","for train_index, test_index in kf.split(incX):\n","    # TODO: define X_train, X_test, Y_train, Y_test\n","\n","\n","    # TODO: define model\n","    model = \"FILL IN HERE\"\n","\n","    # TODO: fit model\n","    # FILL IN HERE\n","\n","    # TODO: compare predictions with actual targets, add accuracy_score to errors\n","    # FILL IN HERE\n","errors /= 5\n","print(errors)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Problem 2c (2 pts):"],"metadata":{"id":"ihTeNsIzdMZ-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Problem 3 (1 pt):\n","How does the depth of a decision tree affect overfitting?\n","\n","\n","###### fill in here"],"metadata":{"id":"o8nWiNQxdTDU"}},{"cell_type":"markdown","metadata":{"id":"c5Roi8_ryiWW"},"source":["## Extra credit: Random Forests (possible + 2 pts)\n","Random Forests are essentially many decision trees combined. The training algorithm for random forests applies the general technique of bootstrap aggregating, or bagging, to tree learners. Given a training set X = x1, ..., xn with responses Y = y1, ..., yn, bagging repeatedly (B times) selects a random sample with replacement of the training set and fits trees to these samples:\n","\n","For b = 1, ..., B:\n","    Sample, with replacement, n training examples from X, Y; call these Xb, Yb.\n","    Train a classification or regression tree fb on Xb, Yb.\n","After training, predictions for unseen samples x' can be made by averaging the predictions from all the individual regression trees on x':\n","\n","Implememnt a Random forest classifier by creating and training 20 decision trees with max_depth 5. Let the predictions be chosen through majority voting on the total training data. Does your model perform better than using a single decision tree?\n","\n","#### **Note: sampling with \"replacement\" is important**  "]},{"cell_type":"code","metadata":{"id":"3ZfQl0d9yiWW"},"source":["import random\n","\n","# Randomize order of training elements for each tree\n","def rand_sample(size):\n","    indices = []\n","    for i in range(size):\n","        indices.append(random.randint(0,size-1))\n","    return indices\n","\n","# Load the whole dataset into X_train and Y_train and initialize a variable tree_preds to contain each tree's prediction\n","X_train = X\n","Y_train = Y\n","tree_preds = []\n","\n","# Create 20 Decision Trees for the lecture 7 dataset\n","for t in range(20):\n","    model = tree.DecisionTreeClassifier(max_depth=5)\n","    sample = rand_sample(df.shape[0])\n","    X_train_tree = X_train.iloc[sample]\n","    Y_train_tree = Y_train.iloc[sample]\n","    # TODO: FILL In Code Here\n","\n","print(\"Accuracy of one decision tree: \", \"FILL IN HERE\")\n","print(\"Accuracy of the random decision forest: \", \"FILL IN HERE\")\n"],"execution_count":null,"outputs":[]}]}