{"cells":[{"cell_type":"markdown","metadata":{"id":"zXFxwASKVCzj"},"source":["# Assignment 7: Linear Classifiers and Model Validation"]},{"cell_type":"markdown","metadata":{"id":"1vwNvAXVYzWC"},"source":["### <b>NetID</b>: PLEASE FILL ME IN"]},{"cell_type":"markdown","metadata":{"id":"zIdbDd6XVCzk"},"source":["##### This week we're introducing linear classifiers, namely the **Perceptron**, and then delving deeper into **Model Validation**."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Imports\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"rki22g-wVCzk"},"source":["## Perceptron\n","\n","Perceptron was developed by American psychologist Frank Rosenblatt in 1957 at the **Cornell** Aeronautical Laboratory. Shout-out to one of our greatest alumni!\n","\n","Perceptron is a linear binary classifier. So the underlying assumption about the dataset is that there are two labels - i.e. binary labels (conventionally, +1 and -1), and that the two classes should be classified with a linear hyperplane (Although, keep in mind that the [Multi-Layer Perceptron (MLP)](https://en.wikipedia.org/wiki/Multilayer_perceptron) is applicable to non-linearly separable data. We won't cover MLP in this course as it is part of Artificial Neural Networks (ANN), which is not in our scope).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"54ykzcCcVCzl"},"source":["The Perceptron \"learns\" a series of weights, each of which corresponds to each input feature, i.e. X of our data. For example, we are given a dataset of dog and cat. The input feature set consists of three columns: weight $x_1$, height $x_2$, and length $x_3$ of each animal. Then Perceptron will keep track of three different weights: $w_1$, $w_2$, and $w_3$. Each pair of input features and weights is multiplied and summed up: $s = w_1*x_1 + w_2*x_2 + w_3*x_3$. If the summed-up result $s$ is greater than a certain threshold, then we predict one class, and if it is less than the threshold, then we predict the other. For example, if our threshold is 0, then we can set it as: if $s > 0$, then the given input feature is a description of (+1) label (i.e. a dog), and if $s < 0$, then it is (-1) label (i.e. a cat). Then Perceptron will check if the predictions made were correct. If some of them were not, then the weights are updated accordingly. This process continues for a certain number of \"epochs,\" or iterations. The end goal is to classify every point correctly by finding a *perfect* linear hyperplane.\n","\n","The final step is to check if our predictions were classified correctly. If they were not, then the weights are updated using a learning rate. This process continues for a certain number of iterations, known as “epochs.” The goal is to determine the weights that produce a linear decision boundary that correctly classifies the predictions.\n","\n","<img src=\"https://cdn-images-1.medium.com/max/1600/1*n6sJ4yZQzwKL9wnF5wnVNg.png\" alt=\"perceptron.png\" style=\"width: 50%;\"/>"]},{"cell_type":"markdown","metadata":{"id":"_iH8kAlpVCz6"},"source":["# Problem 1: Perceptron Learning (4 points)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"i5zFK0LwVCz7"},"source":["Let's create a perceptron to predict whether someone has breast cancer. This is no different from what you've done with models before, but we're going to write our code inside a function so we can reuse it later."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ATy1xsQWVCz8"},"outputs":[],"source":["from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import Perceptron\n","from sklearn.metrics import accuracy_score\n","\n","\"\"\"\n","test_size: a float between 0 and 1 indicating the size of the train set\n","\"\"\"\n","def classifier_accuracy(classifier, features, goal, n_training_points, n_testing_points=None):\n","\n","    if n_training_points > len(goal):\n","        raise ValueError(\"bad input to classifier_accuracy: number of training points requested is greater than length of dataset\")\n","    if n_training_points <= 0:\n","        raise ValueError(\"bad input to classifier_accuracy: number of training points requested must be greater than 0\")\n","\n","    if n_testing_points is None:\n","        n_testing_points = len(goal) - n_training_points\n","\n","    if n_testing_points > len(goal):\n","        raise ValueError(\"bad input to classifier_accuracy: number of testing points requested is greater than length of dataset\")\n","    if n_testing_points < 0:\n","        raise ValueError(\"bad input to classifier_accuracy: number of testing points requested must be greater than 0\")\n","\n","    if n_training_points + n_testing_points > len(goal):\n","        raise ValueError(\"bad input to classifier_accuracy: number of training + testing points requested is greater than length of dataset\")\n","\n","    n_total_points = n_training_points + n_testing_points\n","\n","    if n_training_points + n_testing_points < len(goal):\n","        indices = np.random.choice(len(features), n_total_points)\n","        features = features[indices,:]\n","        goal = goal[indices]\n","\n","    test_size = n_testing_points / n_total_points\n","\n","    ###############################################################################\n","    ##### Don't touch anything in this cell above this line! Only add code below.\n","    ###############################################################################\n","\n","    # TODO Make a train test split with a test size of test_size\n","    # add a line of code here!\n","\n","    # TODO: train the classifier\n","    classifier.fit(\"FILL HERE\", \"FILL HERE\")\n","\n","    #FILL HERE: Compute your model's train and test accuracy using accuracy_score\n","    train_accuracy = None # TODO\n","    test_accuracy = None # TODO\n","\n","    return train_accuracy, test_accuracy\n","\n","\n","X, y = load_breast_cancer(return_X_y=True)\n","# we train on 400 of the ~500 data points, equivalent to 20% test set\n","train, test = classifier_accuracy(Perceptron(), X, y, 400)\n","print(\"train accuracy:\\t\", train)\n","print(\"test accuracy:\\t\", test)\n"]},{"cell_type":"markdown","metadata":{"id":"39ii7Ys4YzWU"},"source":["# Problem 2: Data Limitations of Cross Validation Pt. 1 - Train Size (1 point)"]},{"cell_type":"markdown","metadata":{"id":"5Zjey3u4YzWU"},"source":["Cross validation is an extraordinarily powerful technique and is used in almost every supervised learning problem. But, it does have its limitations -- some problems are inherent to supervised learning, and cross validation can't fix those issues.\n","\n","One such limitation has to do with the size of the train set (i.e. how many points we pass into `model.fit`). Let's explore this limitation."]},{"cell_type":"markdown","metadata":{"id":"dLQzY42iYzWU"},"source":["The code block below may include Python constructs that you're not familiar with. That's okay -- **we don't expect you to be able to understand all the code**, and that's not the point of this example. Just **set n_training_points** by replacing \"FILL IN HERE\", and **analyze the average accuracy** outputted. If you get an error, read the error message and change your number accordingly.\n","\n","**Make sure to try both small (single digits) and big (triple digits) values of n_training_points!**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"afv4jv1rYzWV"},"outputs":[],"source":["X, y = load_breast_cancer(return_X_y=True)\n","\n","# TODO try setting n_training_points to a few different numbers. If you get an error, read the error message and change your number accordingly.\n","n_training_points = None\n","\n","n_testing_points = 100\n","accs = []\n","for i in range(1000):\n","    for _ in range(50):\n","        try:\n","            train_acc, test_acc = classifier_accuracy(Perceptron(), X, y, n_training_points, n_testing_points)\n","            accs.append(test_acc)\n","            break\n","        except ValueError as e:\n","            if \"bad input to classifier_accuracy:\" in str(e):\n","                raise e\n","            continue\n","\n","print(\"avg. accuracy:\", round(np.mean(accs),4))\n"]},{"cell_type":"markdown","metadata":{"id":"BVGoK-yrYzWV"},"source":["## Analysis\n","\n","##### What is the relationship between # of training points and average accuracy?"]},{"cell_type":"markdown","metadata":{"id":"VzqmxUHGYzWV"},"source":["PUT YOUR ANSWER HERE!"]},{"cell_type":"markdown","metadata":{"id":"Xe148x0gYzWW"},"source":["# Problem 3: Kernel (K)lassification (2 points)"]},{"cell_type":"markdown","metadata":{},"source":["Another powerful machine learning model discussed in lecture is the <b>Support Vector Machine</b>, or SVM. These can be used for classification of linear and non-linear data, depending on which kernel function they use. We won't be getting into the math of kernel functions, but if you are interested, [here](https://cs.cornell.edu/courses/cs4780/2024sp/lectures/lecturenote13.html) are Prof. Killian Weinberger's lecture notes on the topic from CS 3780.\n","\n","Let's explore how different kernels are suited to different shaped data!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize the dataset\n","from sklearn.datasets import make_circles\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# extract built-in dataset from sklearn\n","X, y = make_circles(n_samples=300, factor=0.5, noise=0.1, random_state=42)\n","\n","# creating train test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1998)\n","\n","# visualize the raw data\n","plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n","plt.title('Data')\n","plt.show()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# DO NOT EDIT: helper function for seeing decision boundary of model given data X, y\n","def plot_decision_boundary(model, X, y, kernel_type):\n","  # Set the limits of the plot\n","  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","  y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","\n","  # Create a grid of points\n","  xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n","\n","  # Predict the classification for each grid point\n","  Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n","  Z = Z.reshape(xx.shape)\n","\n","  # clear plot\n","  plt.clf()\n","\n","  # Plot the decision boundary\n","  plt.contourf(xx, yy, Z, alpha=0.6, cmap='coolwarm')\n","\n","  # Overlay the actual data points\n","  plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap='coolwarm')\n","\n","  # Add titles and labels\n","  plt.title(f\"Decision Boundary for {kernel_type} Kernel\")\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# use a Linear Kernel\n","svm1 = SVC(kernel='linear')\n","svm1.fit(None, None) # TODO train model\n","linear_pred = svm1.predict(None)  # TODO fill in \n","linear_accuracy = accuracy_score(linear_pred, None)  # TODO fill in correct scores\n","\n","print(f\"Linear SVM Classification Accuracy {linear_accuracy}\")\n","plot_decision_boundary(svm1, X_test, y_test, \"linear\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# use a Radial Basis Function kernel\n","svm2 = SVC(kernel='rbf')      \n","svm2.fit(None, None) # TODO train model\n","rbf_pred = svm2.predict(None)  # TODO fill in \n","rbf_accuracy = accuracy_score(rbf_pred, None)  # TODO\n","\n","print(f\"RBF SVM Classification Accuracy {rbf_accuracy}\")\n","plot_decision_boundary(svm2, X_test, y_test, \"rbf\")"]},{"cell_type":"markdown","metadata":{},"source":["### Which kernel is a better approximator for this data, and why? Hint: Discuss the shape of the data."]},{"cell_type":"markdown","metadata":{},"source":["TODO: YOUR ANSWER HERE!"]},{"cell_type":"markdown","metadata":{"id":"lfDtmxmoYzWY"},"source":["# Problem 4: Selection Bias in Cross Validation (2 points)"]},{"cell_type":"markdown","metadata":{"id":"jdpXa5fqYzWY"},"source":["We use a decision tree to classify try to predict whether a phone both has talk time >= 20 and does not have 3G. We will cover decision trees in more detail in the next lecture. For now, all you need to know is that a decision tree is a type of machine learning model. Based on the accuracy of the model, answer the questions below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OdzetxDLYzWZ"},"outputs":[],"source":["cellphone_df = pd.read_csv(\"phone.csv\")\n","features = cellphone_df[['battery_power']]\n","goal = (cellphone_df[\"talk_time\"] >= 20) & (cellphone_df[\"three_g\"] == False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ru9M_DdYzWZ"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import confusion_matrix\n","from contextlib import suppress\n","\n","accuracies, positive_predictive_values, negative_predictive_values = [],[],[]\n","n_folds = 10\n","kf = KFold(n_folds)\n","for train_index, test_index in kf.split(features):\n","    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n","    y_train, y_test = goal[train_index], goal[test_index]\n","    tree = DecisionTreeClassifier()\n","    tree.fit(X_train, y_train)\n","    pred = tree.predict(X_test)\n","\n","    #true negative, false positive, false negative, true positive\n","    tn, fp, fn, tp = confusion_matrix(y_test, pred, labels=[False,True]).ravel()\n","\n","    accuracies.append((tn + tp) / len(y_test))\n","    if tp + fn > 0:\n","        positive_predictive_values.append(tp / (tp + fn))\n","    if tn + fp > 0:\n","        negative_predictive_values.append(tn / (tn + fp))\n","\n","cv_accuracy = np.mean(accuracies)\n","\n","print(\"Accuracy:\", cv_accuracy)"]},{"cell_type":"markdown","metadata":{"id":"0LdsUYvzYzWZ"},"source":["That's an amazing accuracy! Now, let's look closer at our accuracy breakdown.\n","\n","We'll look at the **positive predictive value** and the **negative predictive value**. The positive predictive value is the accuracy score if we only look at samples with a true label of \"True\", and the negative predictive value is the accuracy score if we only look at samples with a true label of \"False\".\n","\n","So, the question _\"Out of the phones that have a talk time >= 20 but don't have 3G, what percent do we predict correctly?\"_ corresponds to the positive predictive value."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bg2IDqBwYzWZ"},"outputs":[],"source":["cv_positive_predictive_value = np.mean(positive_predictive_values)\n","cv_negative_predictive_value = np.mean(negative_predictive_values)\n","\n","print(\"accuracy for positive samples\",  cv_positive_predictive_value)\n","print(\"accuracy for negative samples\", cv_negative_predictive_value)"]},{"cell_type":"markdown","metadata":{"id":"-_Xnt7b4YzWZ"},"source":["### Uh oh. We have a positive predictive value of 0%. How could this be, when our accuracy score is so high?\n","\n","#### _Hint 1_: We use just `battery_power` to predict, and get a 98% accuracy. That seems wrong. How did our model have such a high accuracy? What contributed to that accuracy?\n","\n","##### _Hint 2_: How do our predictions look? How does the goal look?"]},{"cell_type":"markdown","metadata":{"id":"wjT_qD35YzWa"},"source":["FILL IN ANSWER HERE!"]},{"cell_type":"markdown","metadata":{"id":"r-elAzxsYzWa"},"source":["# Problem 5: A Common Mistake (1 pt)"]},{"cell_type":"markdown","metadata":{"id":"nHV_YVUPYzWa"},"source":["The following piece of code contains a (fatal) issue. Find it! (No need to correct the error -- just state what it is.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iDKhBWL7YzWa"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import root_mean_squared_error\n","\n","X, y = load_breast_cancer(return_X_y=True)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","\n","KNN = KNeighborsClassifier()\n","KNN.fit(X_train, y_train)\n","\n","y_pred = KNN.predict(X_test)\n","\n","error = root_mean_squared_error(y_pred, y_test, squared=True)\n","\n","print(\"Error:\", error)"]},{"cell_type":"markdown","metadata":{"id":"ycIB6V8BYzWb"},"source":["### What's wrong?\n","\n","##### _Hint: Think about what error is appropriate to use in regression vs. classification problems._\n"]},{"cell_type":"markdown","metadata":{},"source":["FILL IN YOUR ANSWER HERE"]}],"metadata":{"colab":{"collapsed_sections":["zXFxwASKVCzj","_iH8kAlpVCz6","39ii7Ys4YzWU","Xe148x0gYzWW","XGFT79V9YzWY","lfDtmxmoYzWY","r-elAzxsYzWa","VDh3HDmHYzWb"],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"notebookId":"PAG=GBt`Q~yB`["},"nbformat":4,"nbformat_minor":0}
