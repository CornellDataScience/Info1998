{"cells":[{"cell_type":"markdown","metadata":{"id":"zXFxwASKVCzj"},"source":["# Assignment 7: Linear Classifiers and Model Validation"]},{"cell_type":"markdown","metadata":{"id":"1vwNvAXVYzWC"},"source":["### Net Id: FILL ME IN"]},{"cell_type":"markdown","metadata":{"id":"zIdbDd6XVCzk"},"source":["##### This week we're introducing linear classifiers, namely the **Perceptron**, and then delving deeper into **Model Validation**."]},{"cell_type":"markdown","metadata":{"id":"rki22g-wVCzk"},"source":["## Perceptron\n","\n","Perceptron was developed by American psychologist Frank Rosenblatt in 1957 at the **Cornell** Aeronautical Laboratory. Shout-out to one of our greatest alumni!\n","\n","Perceptron is a linear binary classifier. So the underlying assumption about the dataset is that there are two labels - i.e. binary labels (conventionally, +1 and -1), and that the two classes should be classified with a linear hyperplane (Although, keep in mind that the [Multi-Layer Perceptron (MLP)](https://en.wikipedia.org/wiki/Multilayer_perceptron) is applicable to non-linearly separable data. We won't cover MLP in this course as it is part of Artificial Neural Networks (ANN), which is not in our scope).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"54ykzcCcVCzl"},"source":["The Perceptron \"learns\" a series of weights, each of which corresponds to each input feature, i.e. X of our data. For example, we are given a dataset of dog and cat. The input feature set consists of three columns: weight $x_1$, height $x_2$, and length $x_3$ of each animal. Then Perceptron will keep track of three different weights: $w_1$, $w_2$, and $w_3$. Each pair of input features and weights is multiplied and summed up: $s = w_1*x_1 + w_2*x_2 + w_3*x_3$. If the summed-up result $s$ is greater than a certain threshold, then we predict one class, and if it is less than the threshold, then we predict the other. For example, if our threshold is 0, then we can set it as: if $s > 0$, then the given input feature is a description of (+1) label (i.e. a dog), and if $s < 0$, then it is (-1) label (i.e. a cat). Then Perceptron will check if the predictions made were correct. If some of them were not, then the weights are updated accordingly. This process continues for a certain number of \"epochs,\" or iterations. The end goal is to classify every point correctly by finding a *perfect* linear hyperplane.\n","\n","The final step is to check if our predictions were classified correctly. If they were not, then the weights are updated using a learning rate. This process continues for a certain number of iterations, known as “epochs.” The goal is to determine the weights that produce a linear decision boundary that correctly classifies the predictions.\n","\n","<img src=\"https://cdn-images-1.medium.com/max/1600/1*n6sJ4yZQzwKL9wnF5wnVNg.png\" alt=\"perceptron.png\" style=\"width: 50%;\"/>"]},{"cell_type":"markdown","metadata":{"id":"_iH8kAlpVCz6"},"source":["# Problem 1: Perceptron Learning (4 points)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"i5zFK0LwVCz7"},"source":["Let's create a perceptron to predict whether someone has breast cancer. This is no different from what you've done with models before, but we're going to write our code inside a function so we can reuse it later."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ATy1xsQWVCz8"},"outputs":[],"source":["from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import Perceptron\n","from sklearn.metrics import accuracy_score\n","\n","\"\"\"\n","test_size: a float between 0 and 1 indicating the size of the train set\n","\"\"\"\n","def classifier_accuracy(classifier, features, goal, n_training_points, n_testing_points=None):\n","\n","    if n_training_points > len(goal):\n","        raise ValueError(\"bad input to classifier_accuracy: number of training points requested is greater than length of dataset\")\n","    if n_training_points <= 0:\n","        raise ValueError(\"bad input to classifier_accuracy: number of training points requested must be greater than 0\")\n","\n","    if n_testing_points is None:\n","        n_testing_points = len(goal) - n_training_points\n","\n","    if n_testing_points > len(goal):\n","        raise ValueError(\"bad input to classifier_accuracy: number of testing points requested is greater than length of dataset\")\n","    if n_testing_points < 0:\n","        raise ValueError(\"bad input to classifier_accuracy: number of testing points requested must be greater than 0\")\n","\n","    if n_training_points + n_testing_points > len(goal):\n","        raise ValueError(\"bad input to classifier_accuracy: number of training + testing points requested is greater than length of dataset\")\n","\n","    n_total_points = n_training_points + n_testing_points\n","\n","    if n_training_points + n_testing_points < len(goal):\n","        indices = np.random.choice(len(features), n_total_points)\n","        features = features[indices,:]\n","        goal = goal[indices]\n","\n","    test_size = n_testing_points / n_total_points\n","\n","    ###############################################################################\n","    ##### Don't touch anything in this cell above this line! Only add code below.\n","    ###############################################################################\n","\n","    #FILL HERE: Make a train test split with a test size of test_size\n","\n","\n","    #FILL HERE: train the classifier\n","    classifier.fit(\"FILL HERE\", \"FILL HERE\")\n","\n","\n","    #FILL HERE: Compute your model's train and test accuracy using accuracy_score\n","    train_accuracy =\n","    test_accuracy =\n","\n","    return train_accuracy, test_accuracy\n","\n","\n","X, y = load_breast_cancer(return_X_y=True)\n","# we train on 400 of the ~500 data points, equivalent to 20% test set\n","train, test = classifier_accuracy(Perceptron(), X, y, 400)\n","print(\"train accuracy:\\t\", train)\n","print(\"test accuracy:\\t\", test)\n"]},{"cell_type":"markdown","metadata":{"id":"39ii7Ys4YzWU"},"source":["# Problem 2: Data Limitations of Cross Validation Pt. 1 - Train Size (1 point)"]},{"cell_type":"markdown","metadata":{"id":"5Zjey3u4YzWU"},"source":["Cross validation is an extraordinarily powerful technique and is used in almost every supervised learning problem. But, it does have its limitations -- some problems are inherent to supervised learning, and cross validation can't fix those issues.\n","\n","One such limitation has to do with the size of the train set (i.e. how many points we pass into `model.fit`). Let's explore this limitation."]},{"cell_type":"markdown","metadata":{"id":"dLQzY42iYzWU"},"source":["The code block below may include Python constructs that you're not familiar with. That's okay -- **we don't expect you to be able to understand all the code**, and that's not the point of this example. Just **set n_training_points** by replacing \"FILL IN HERE\", and **analyze the average accuracy** outputted. If you get an error, read the error message and change your number accordingly.\n","\n","**Make sure to try both small (single digits) and big (triple digits) values of n_training_points!**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"afv4jv1rYzWV"},"outputs":[],"source":["X, y = load_breast_cancer(return_X_y=True)\n","\n","# TODO try setting n_training_points to a few different numbers. If you get an error, read the error message and change your number accordingly.\n","n_training_points = \"FILL IN HERE\"\n","\n","n_testing_points = 100\n","accs = []\n","for i in range(1000):\n","    for _ in range(50):\n","        try:\n","            train_acc, test_acc = classifier_accuracy(Perceptron(), X, y, n_training_points, n_testing_points)\n","            accs.append(test_acc)\n","            break\n","        except ValueError as e:\n","            if \"bad input to classifier_accuracy:\" in str(e):\n","                raise e\n","            continue\n","\n","print(\"avg. accuracy:\", round(np.mean(accs),4))\n"]},{"cell_type":"markdown","metadata":{"id":"BVGoK-yrYzWV"},"source":["## Analysis\n","\n","##### What is the relationship between # of training points and average accuracy?"]},{"cell_type":"markdown","metadata":{"id":"VzqmxUHGYzWV"},"source":[]},{"cell_type":"markdown","metadata":{"id":"Xe148x0gYzWW"},"source":["# Problem 3: Data Limitations of Cross Validation Pt. 2 - Test Set Size (3 points)"]},{"cell_type":"markdown","metadata":{"id":"a_XWB1DCYzWW"},"source":["What about test size (i.e. how many points we pass into our scoring function)? Do we also need a large test set?\n","\n","Let's see what happens when we vary the size of the test set.\n","\n","**In this problem, you will produce and analyze some scatterplots. We've provided most of the code; the bulk of your work will be in interpreting graphs.**\n","\n","The dataset is about phone prices. We're trying to classify each phone into one of four price bins."]},{"cell_type":"markdown","metadata":{"id":"HGiJGxvsYzWW"},"source":["The following block of code loops through test sizes varying from from 5 points to 50 points (with a fixed number of training points). For _each_ of these test sizes, it runs classifier_accuracy 500 times and finds summary statistics of those 500 runs. Recall that this is similar to what we did with cross-validation, except instead of producing one value, we're looping through different test sizes and producing a value for each test size."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mGgXJf-PYzWW"},"outputs":[],"source":["from scipy import stats\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# setting up the configuration\n","num_iters_per_test_size = 500 # for each test size, we'll collect 500 accuracies\n","test_sizes = list(range(5,50,5))\n","cellphone_df = pd.read_csv(\"phone.csv\")\n","goal = cellphone_df[\"price_range\"]\n","features = cellphone_df.drop(columns=[\"price_range\"]).to_numpy()\n","\n","# will store summary statistics about train and test accuracy\n","train_acc_stats = []\n","test_acc_stats = []\n","for test_size in test_sizes:\n","    tree = DecisionTreeClassifier()\n","    accs = [classifier_accuracy(tree, features, goal, 1500, test_size) for i in range(num_iters_per_test_size)]\n","    train_acc_stats.append(stats.describe([x[0] for x in accs])) # train accuracy statistics\n","    test_acc_stats.append(stats.describe([x[1] for x in accs])) # test accuracy statistics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZYT9rN9nYzWX"},"outputs":[],"source":["\"\"\"\n","Pass in one of the following:\n","- \"nobs\"\n","- \"min\"\n","- \"max\"\n","- \"mean\"\n","- \"variance\"\n","- \"kurtosis\"\n","\n","Returns two arrays: one for train accuracy statistics and one for test accuracy statistics.\n","For example, get_summary_statistics(\"mean\") will return:\n","1) the cross-validation train accuracy for each test size in `test_sizes`\n","2) the cross-validation test accuracy for each test size in `test_sizes`\n","\"\"\"\n","def get_summary_statistics(summary_statistic_name):\n","    index = 0\n","    if summary_statistic_name == \"nobs\":\n","        index = 0\n","    elif summary_statistic_name == \"min\":\n","        index = 1\n","    elif summary_statistic_name == \"max\":\n","        index = 1\n","    elif summary_statistic_name == \"mean\":\n","        index = 2\n","    elif summary_statistic_name == \"variance\":\n","        index = 3\n","    elif summary_statistic_name == \"kurtosis\":\n","        index = 5\n","    elif summary_statistic_name == \"constant\":\n","        return [1 for i in train_acc_stats], [1 for i in test_acc_stats]\n","    else:\n","        return None, None\n","    train_statistics = [stat[index] for stat in train_acc_stats]\n","    test_statistics = [stat[index] for stat in test_acc_stats]\n","    if summary_statistic_name == \"min\":\n","        train_statistics = [minmax[0] for minmax in train_statistics]\n","        test_statistics = [minmax[0] for minmax in test_statistics]\n","    elif summary_statistic_name == \"max\":\n","        train_statistics = [minmax[1] for minmax in train_statistics]\n","        test_statistics = [minmax[1] for minmax in test_statistics]\n","    return train_statistics, test_statistics"]},{"cell_type":"markdown","metadata":{"id":"2CAKVE-HYzWX"},"source":["Below is some starter code for producing the scatterplots. All you need to do is change \"constant\" to one of the following summary statistic names:\n","- \"nobs\"\n","- \"min\"\n","- \"max\"\n","- \"mean\"\n","- \"variance\"\n","- \"kurtosis\"\n","\n","and that summary statistic will be graphed below for different test sizes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jFZkvDC3YzWX"},"outputs":[],"source":["summary_statistic_name = \"constant\" # TODO replace this with the name of a summary statistic\n","\n","train_yaxis, test_yaxis = get_summary_statistics(summary_statistic_name)\n","\n","if train_yaxis is None:\n","    print(\"\\\"\" + summary_statistic_name + \"\\\" is not a valid statistic name! Make sure you spelled it correctly\")\n","else:\n","    fig, axes = plt.subplots(1,2, sharex=True, sharey=True)\n","    fig.set_figwidth(15)\n","    fig.subplots_adjust(hspace=0.5)\n","    # fig.\n","    axes[0].scatter(test_sizes,train_yaxis)\n","    axes[0].set_title(\"Train accuracy \" + summary_statistic_name, fontsize=16)\n","    axes[0].set_xlabel(\"Test size\", fontsize=16)\n","    axes[0].set_ylabel(summary_statistic_name, fontsize=16)\n","\n","    axes[1].scatter(test_sizes,test_yaxis)\n","    axes[1].set_title(\"Test accuracy \" + summary_statistic_name, fontsize=16)\n","    axes[1].set_xlabel(\"Test size\", fontsize=16)\n","    axes[1].set_ylabel(summary_statistic_name, fontsize=16)\n","\n","    plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"PILtWtdyYzWX"},"source":["## Analysis\n","\n","##### Which is better, a small test set or a large test set? Why is it better? What could be causing this problem in small/large test sets?\n"]},{"cell_type":"markdown","metadata":{"id":"Ep_rmCVZYzWY"},"source":[]},{"cell_type":"markdown","metadata":{"id":"lfDtmxmoYzWY"},"source":["# Problem 4: Selection Bias in Cross Validation (2 points)"]},{"cell_type":"markdown","metadata":{"id":"jdpXa5fqYzWY"},"source":["Below, we use a decision tree to classify try to predict whether a phone both has talk time >= 20 and does not have 3G."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OdzetxDLYzWZ"},"outputs":[],"source":["cellphone_df = pd.read_csv(\"phone.csv\")\n","features = cellphone_df[['battery_power']]\n","goal = (cellphone_df[\"talk_time\"] >= 20) & (cellphone_df[\"three_g\"] == False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ru9M_DdYzWZ"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import confusion_matrix\n","from contextlib import suppress\n","\n","accuracies, positive_predictive_values, negative_predictive_values = [],[],[]\n","n_folds = 10\n","kf = KFold(n_folds)\n","for train_index, test_index in kf.split(features):\n","    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n","    y_train, y_test = goal[train_index], goal[test_index]\n","    tree = DecisionTreeClassifier()\n","    tree.fit(X_train, y_train)\n","    pred = tree.predict(X_test)\n","\n","    #true negative, false positive, false negative, true positive\n","    tn, fp, fn, tp = confusion_matrix(y_test, pred, labels=[False,True]).ravel()\n","\n","    accuracies.append((tn + tp) / len(y_test))\n","    if tp + fn > 0:\n","        positive_predictive_values.append(tp / (tp + fn))\n","    if tn + fp > 0:\n","        negative_predictive_values.append(tn / (tn + fp))\n","\n","cv_accuracy = np.mean(accuracies)\n","\n","print(\"Accuracy:\", cv_accuracy)"]},{"cell_type":"markdown","metadata":{"id":"0LdsUYvzYzWZ"},"source":["That's an amazing accuracy! Now, let's look closer at our accuracy breakdown.\n","\n","We'll look at the **positive predictive value** and the **negative predictive value**. The positive predictive value is the accuracy score if we only look at samples with a true label of \"True\", and the negative predictive value is the accuracy score if we only look at samples with a true label of \"False\".\n","\n","So, the question _\"Out of the phones that have a talk time >= 20 but don't have 3G, what percent do we predict correctly?\"_ corresponds to the positive predictive value."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bg2IDqBwYzWZ"},"outputs":[],"source":["cv_positive_predictive_value = np.mean(positive_predictive_values)\n","cv_negative_predictive_value = np.mean(negative_predictive_values)\n","\n","print(\"accuracy for positive samples\",  cv_positive_predictive_value)\n","print(\"accuracy for negative samples\", cv_negative_predictive_value)"]},{"cell_type":"markdown","metadata":{"id":"-_Xnt7b4YzWZ"},"source":["### Uh oh. We have a positive predictive value of 0%. How could this be, when our accuracy score is so high?\n","\n","_Hint 1_: We use just `battery_power` to predict, and get a 98% accuracy. That seems wrong. How did our model have such a high accuracy? What contributed to that accuracy?\n","\n","_Hint 2_: How do our predictions look? How does the goal look?"]},{"cell_type":"markdown","metadata":{"id":"wjT_qD35YzWa"},"source":[]},{"cell_type":"markdown","metadata":{"id":"r-elAzxsYzWa"},"source":["# Problem 5: A Common Mistake (1 pt)"]},{"cell_type":"markdown","metadata":{"id":"nHV_YVUPYzWa"},"source":["The following piece of code contains a (fatal) issue. Find it! (No need to correct the error -- just state what it is.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iDKhBWL7YzWa"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import root_mean_squared_error\n","\n","X, y = load_breast_cancer(return_X_y=True)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","\n","KNN = KNeighborsClassifier()\n","KNN.fit(X_train, y_train)\n","\n","y_pred = KNN.predict(X_test)\n","\n","error = root_mean_squared_error(y_pred, y_test, squared=True)\n","\n","print(\"Error:\", error)"]},{"cell_type":"markdown","metadata":{"id":"ycIB6V8BYzWb"},"source":["### What's wrong?\n","\n","_Hint: Think about what error is appropriate to use in regression vs. classification problems._\n"]}],"metadata":{"colab":{"collapsed_sections":["zXFxwASKVCzj","_iH8kAlpVCz6","39ii7Ys4YzWU","Xe148x0gYzWW","XGFT79V9YzWY","lfDtmxmoYzWY","r-elAzxsYzWa","VDh3HDmHYzWb"],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"notebookId":"PAG=GBt`Q~yB`["},"nbformat":4,"nbformat_minor":0}
