
# Fundamentals of Machine Learning


## NETID: FILL IN HERE

### Problems
- Problem 1 (1 point)
- Problem 2 (4 points)
- Problem 3 (2 points)
- Problem 4 (3 points)


```{r}
# reading in our data
df = read.csv('lecture4data.csv')
```


## Problem 1 (1 point):
```{r}
problem_1_data = df
```


#### a) Print out the first 5 rows of `problem_1_data`

```{r}
#FILL IN HERE
```

#### b) add a new column to `problem_1_data` called `education_profit_ratio` that holds each school's `median_earnings` divided by it's `cost`
```{r}
#FILL IN HERE
```

#### c) (OPTIONAL) Briefly describe what `cost_single_brackets`, and `cost_double_brackets` look like (what their shapes/dimensions are).

*Hint*: You can use R's `typeof` function to tell the type of an objective. You can also try printing out the object to see what it looks like. You can also take a look in the environment where all the variables are stored for more information. 

```{r}
cost_single_brackets = problem_1_data['cost']
cost_double_brackets = problem_1_data[['cost']]
```


## Basics of Machine Learning

So far we have discussed the importance of data visualization and data preprocessing. These steps are important in preparing the data sets. Now, we will start our machine learning section of the course. We have already discussed how a machine learning model is like a function, and we will now explain a bit more using *linear regression* models.


This dataset contains information on different colleges across the US. The full dataset was assembled from https://collegescorecard.ed.gov/data/ and this version was trimmed down to a few important columns and cleaned to removed schools with missing data.


We will look at adm_rate (admission rate), undergrads (number of undergrads), cost (cost to attand the college for a year), compl_4 (percent of students graduated in 4 years), median_hh_inc (median household income of students), and median_earnings (median earnings 10 years after starting college).


We want to predict median_earnings using other columns in the data set. In other words, we have some inputs from the dataframe (such as cost), and we want to output another column in the dataframe (median_earnings).

## Section 1: Intro

To keep things clean throughout this notebook, in each section, we first make a copy of the original data. You won't have to do this in your own projects.

```{r}
section_1_data = df
head(section_1_data)
```

Now, let's start machine learning! We'll choose a set of inputs, and then use machine learning to predict median_earnings using that set of inputs.

Here is where R differs a little from python. We don't need to import any extra libraries or anything to run linear regression. The function is very straightforward and easy to use.

Our first step is to partition out our training and test sets. We want to split the data into two sets, one to train the model on and one to test the model with. Before we do this we can set a seed. Partitioning the data set is a random process and the seed allows it to preform the same on your computer as mine. You can remove this for any real world applications. 

```{r}
set.seed(42)
```


We will use a 80/20 split (80% of the data used for training, 20% for testing)
```{r}
training_row_ind = sample(1:nrow(section_1_data), 0.8*nrow(section_1_data))  # selects row indices for training data
training_df = section_1_data[training_row_ind, ]  # training data
test_df = section_1_data[-training_row_ind, ]   # test data
```

Now we fit our linear model on the training data:
```{r}
linear_reg_model = lm(median_earnings~cost, data = training_df)
```

The lm function general format if `lm(target ~ var_1 + var_2 + ... + var_n, data=train_set)` where the target id the output/response variable and the vars are the inputs/predictors.

```{r}
#now we can predict some values with our model
preds = predict(linear_reg_model,test_df)
```

Let's take a look at how our predicted values compare to the actual values
```{r}
compare = cbind(test_df$median_earnings, preds)
colnames(compare) = c("Actual", "Predicted")
compare = as.data.frame(compare)
head(compare)
```


One way we can asses the success of our model is to look at the $R^2$ value (same as the python .score() function for linear regression). It is a measure of how much variation in the response variable is captured by the predictor(s). We want it to be as close to 1 as possible (if it's exactly 1, something fishy is happening) and definitely not negative.

We can get this value
```{r}
#summary(linear_reg_model) # this line gives the whole summary output if you want to take a look, you don't need to understand it all quite yet
summary(linear_reg_model)$r.squared
```

Well our $R^2$ value isn't great..but that's okay because we are fitting a relatively simple model here so we don't expect to get great performance right away. 

Let's take a look at what we've done so far
```{r}
plot(section_1_data$cost,section_1_data$median_earnings, col = "blue",xlab = "Cost", ylab = "Median Earnings", pch = 16)
points(test_df$cost,preds, pch = 16)
legend(9000,120000,legend=c("Predicted", "Actual"), col=c("Black", "Blue"), cex = 0.8, pch = c(16,16))
```
The blue points are the actual data and the black points (in a line) are the predicted points. 


### Summary: 

We fit our model on a portion of our data set. We call the predict function, giving it our model and the test data and it gives us predicted outputs. 

### What we don't know:

What is "fitting"?

Why did we split the data set?

How does the model make predictions?

How do we increase the model's score (how do we make our model better)?

## Section 2: Fitting / Training

The inputs are called the "features" and the output is called the "target"

The linear regression is our "model"

In reality, we will train a model on known data, and that model is our product -- we're done with it, and we ship it out to be used. The user will give it new data, and the model will predict the corresponding outputs.
We emulate that by splitting our data into "known" data and "new" data. There are a lot of synonyms for known data and new data, and we'll introduce a couple today.

The `lm()` function fits our linear model on the variables we give it and gives us that model. We then can use the predict function with our model and a test set to create some predictions. 

What we sill don't know:

How does the model make predictions?

How do we increase the model's score (how do we make our model better)?

## Section 3: Models

```{r}
section_3_data = df

set.seed(42)

train = sample(1:nrow(section_3_data), 0.8*nrow(section_3_data))
train_df = section_3_data[training_row_ind, ] 
test_df = section_3_data[-training_row_ind, ] 

model3 = lm(median_earnings ~ cost + adm_rate, data = train_df)
```

```{r}
model3$coefficients
```

`model3` is a linear regression model of the form $y = B_0 + B_1*x_1 + B_2*x_2$, where:

$y = $ median_earnings, our target


$x_1 = $ cost


$B_1 = 0.311348$, the coefficient on cost


$x_2 =$ admission rate


$B_2 = -7555.243043$, the coefficient on admission rate


$B_0 = 36500.942564$, the intercept. 


We won't talk too much about the intercept in this class, but if you're interested in learning more, feel free to ask us after class.

Say you have information about a school's cost and admission rate. Then, `model3` can use the school's cost and admission rate to predict the school's median earnings. How does it do that? It calculates $predicted\_median\_earnings = B_1*x_1 + B_2 * x_2$

Let's try this out on an example.

```{r}
example_cost = 22232
example_adm_rate = 0.6043

B1 = model3$coefficients[[2]] #double brackets here remove the name label attached to the value (leftover from the lm function)
B2 = model3$coefficients[[3]]
B0 = model3$coefficients[[1]]

manually_predicted_earnings = B0 + B1*example_cost + B2*example_adm_rate
manually_predicted_earnings
```

```{r}
new = data.frame(cost = c(example_cost), adm_rate = c(example_adm_rate))
predict(model3, new)
```

The two results were the same. From this, we can conclude that `predict` is really just outputting $B_0 + B_1*x_1 + B_2*x_2$.

(When we look at other algorithms in later lectures, the equation will be different. However, the general idea is the same)


### What we still don't know

How do we increase the model's score (how do we make our model better)?

## Problem 2 (4 points):

Use 'cost' and 'compl_4' to predict 'median_earnings'.

Use a train test split of 25% as a test set (not 20%, which all the previous examples used).

Print out the median_earnings predicted for the test set. Also print out the model's score.

```{r}
problem_2_data = df
```


```{r}
# fill in here

# create the train and test sets


# run the lm function


# get and print the score (R^2) value


# get and print the predictions
```

## Section 4: Validation and Testing

```{r}
section_4_data = subset(df, select=-c(name, city)) # we will use all variables except name, this line of code drops those two columns

train = sample(1:nrow(section_4_data), 0.80*nrow(section_4_data))
train_df = section_4_data[train,]
test_df = section_4_data[-train,]

model4 = lm(median_earnings~., data = train_df)

preds4 = predict(model4,test_df)

#R^2 value
summary(model4)$r.squared
```


```{r}
#training Mean Squared Error
train_preds = predict(model4, train_df)
train_error = train_df$median_earnings - train_preds
mse_train = mean(train_error^2)
mse_train
#Testing Mean Squared Error
test_error = test_df$median_earnings - preds4
mse_test = mean(test_error^2)
mse_test
```

We will touch on Mean Squared Error more next class so don't worry too much about it right now, but it provides a good general feel to how well your model is preforming on the test set opposed to the train set. 

## Problem 3 (2 points):

Fill in the rest of the code to create a Train-Validate-Test split, fit the model and get the train, validate, and test MSEs
```{r}
# I will give you the sample line here:
splitSample = sample(1:3, size=nrow(section_4_data), prob=c(0.7,0.15,0.15), replace = TRUE)

#I will also fill this in here for you but I suggest looking at the splitSample object to see why I did this
train_df = section_4_data[splitSample==1,]
validate_df = section_4_data[splitSample==2,]
test_df = section_4_data[splitSample==3,]

model = lm() # FILL IN HERE

#FILL IN HERE
pred_train = 
pred_val = 
pred_test =
  
#Training MSE
train_error = 
mse_train = 
  
#Validate MSE
validate_error = 
mse_val = 
  
#Testing MSE
test_error = 
mse_test = 

```


## Problem 4 (3 points):

Use 'median_earnings2' to predict 'median_earnings' (median_earnings2 is just a copy of the median_earnings column)

Remember to create a train-test split (use whatever percentage of test set you want).

Print out the coefficients. Print out the model's $R^2$. Briefly explain why you got the coefficients and r-squared that you got (The R output gives you a little nudge in the right direction here).

```{r}
problem_4_data = df
problem_4_data$median_earnings2 = problem_4_data$median_earnings

#split data


#fit and print coeffs


#print r^2
```




## Problem 5: BONUS

Linear regression does well with variables that are correlated. Look up how to print out the correlation matrix (don't forget to exclude nominal variables).

Looking at the correlation matrix, pick at least 3 features that have a low (between -0.5 and 0.5) correlation with median_earnings and plot scatterplots of each of them against median_earnings.
Using these scatterplot, pick one of the features and try a few transformations that might increase its correlation with median_earnings. That is, create new features (columns) and see what their correlation with median_earnings is.

Create a model using one of these new features, as well as other features that are well correlated with median_earnings, as features. Print out the score.


