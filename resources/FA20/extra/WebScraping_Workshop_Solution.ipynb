{
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.3"
        },
        "notebookId": "^EG=gSDql5SN"
    },
    "nbformat": 4,
    "nbformat_minor": 2,
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Webscraping Workshop\n",
                "#### Cornell Data Science\n",
                "##### Author: Christopher Elliott"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Installation Guide\n",
                "Before starting the guide: the lastest release of Anaconda should install all of the modules used in this guide **except** selenium. You **MUST** install selenium to use many parts of this guide. If selenium is not installed, some modules will **NOT** work. to install selenium run `pip install selenium` in your terminal or command prompt. Additonally, download a webdriver to drive selenium. During this workshop and guide, we will use Google Chrome's driver. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Beautiful Soup\n",
                "import bs4\n",
                "from bs4 import BeautifulStoneSoup\n",
                "from requests import get\n",
                "import urllib\n",
                "\n",
                "#bs4 requests urllib"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Beautiful Soup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Recruitment NoticeOur recruitment cycle for Spring 2021 has started! Fill out our application via Google Forms by clicking the \u201cApplication Form\u201d button below. Or, join our Information Sessions on Wednesday, October 7 or Tuesday, October 13 at 6PM EST by clicking on the \u201cInfo Session Zoom\u201d button below. Hope to see you soon!\n"
                    ]
                }
            ],
            "source": [
                "#Asking for information from server\n",
                "url ='https://cornelldata.science'\n",
                "response = get(url)\n",
                "soup = bs4.BeautifulSoup(response.text,'html.parser')\n",
                "#print(soup.prettify)\n",
                "\n",
                "#Basic overview\n",
                "about = soup.find_all('div',{'class':'sqs-block-content'})\n",
                "print(about[1].text)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "textual information \n",
                        "\n",
                        "\n",
                        "Experience\n",
                        "We work on projects that span the spectrum of data science. From applied deep learning to user-friendly visualizations, there is probably someone in CDS working on it.\n",
                        "\n",
                        "About Projects\n",
                        "\n",
                        "\n",
                        "Hyperlink text \n",
                        "\n",
                        "/overview\n"
                    ]
                }
            ],
            "source": [
                "# div groups can have multiple tags we can capture at once\n",
                "\n",
                "#Text inforamtion\n",
                "experience = soup.find('div',{'class':'image-card sqs-dynamic-text-container'})\n",
                "print('textual information \\n')\n",
                "print(experience.text)\n",
                "\n",
                "# We know that this div group has textual information but also contains information about weblinks\n",
                "# We can get these links by searching for the hyperlink HTML tag <a>. \n",
                "# After getting the tag, we then can indicate there must also be a hyperlink reference in our search query\n",
                "# once doing that, we can then get the link that the website goes to \n",
                "print(\"Hyperlink text \\n\")\n",
                "link = experience.find('a',href=True)['href']\n",
                "print(link)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As you can see, the hyperlink text only outputs `/overview`. This is because it is standard practice in web development to only link to websites using thier webpage extension if the webpage is on the same website. To go to the next web page, simple add the hyperlink text to the original url."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "https://cornelldata.science/overview\n"
                    ]
                }
            ],
            "source": [
                "new_url = url+link\n",
                "print(new_url)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now that we have this new link, we can extract more information from the next webpage. We've already gone over how to extract text and hyperlinks from webpages So let's now talk about how to extract and download images."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "('webscraped.jpg', <http.client.HTTPMessage at 0x10697c048>)"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "response = get('https://cornelldata.science/education')\n",
                "soup = bs4.BeautifulSoup(response.text,'html.parser')\n",
                "image = soup.find('div', {'class':'image-block-outer-wrapper layout-caption-below design-layout-inline combination-animation-none individual-animation-none individual-text-animation-none'})\n",
                "image_address = image.img['src']\n",
                "urllib.request.urlretrieve(image_address,'webscraped.jpg')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "After you run that script, you should notice that there is now a picture called `webscraped.jpg` in your folder. This is an easy way to download many pictures quickly on the internet. Now that you have some understanding of webscraping. Try to scrape the overview text and image of Tanmay from the INFO 1998 course website. `Hint: you can use the id tag of HTML websites to select an individual div tag`. \n",
                "https://cornelldata.science/people"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Text: I joined CDS because it offered me an opportunity to give back to the Cornell community, by means of teaching a class that I once took and that I credit with introducing me to the realm of data science. In my free time, I love beatboxing, listening to hip-hop music, and (re)watching TV shows.\n"
                    ]
                }
            ],
            "source": [
                "response = get('https://cornelldata.science/people')\n",
                "soup = bs4.BeautifulSoup(response.text,'html.parser')\n",
                "tanmay = soup.find(id='block-a83539e52282ec910c9b')\n",
                "tanmay_image = tanmay.img\n",
                "tanmay_bio_text = None\n",
                "for el in tanmay.descendants:\n",
                "    if isinstance(el, bs4.element.Tag):\n",
                "        tanmay_bio_text = el\n",
                "\n",
                "urllib.request.urlretrieve(tanmay_image[\"src\"],'webscraped_tanmay_image.jpg')\n",
                "print(\"Text:\", tanmay_bio_text.get_text())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ]
}