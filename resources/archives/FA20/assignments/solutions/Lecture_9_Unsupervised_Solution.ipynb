{
    "metadata": {
        "colab": {
            "name": "Lecture 9 - Solutions SP2020.ipynb",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.3"
        },
        "notebookId": "^EG=G=gSDql5SN"
    },
    "nbformat": 4,
    "nbformat_minor": 1,
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "T6JQplYgF-jg"
            },
            "source": [
                "# Unsupervised Learning"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "RbHmglpHF-jh"
            },
            "source": [
                "Previously, we have been focusing on supervised learning. Recall that we covered two types of supervised learning models: regression for predicting continuous variables and classification for predicting categorical ones."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "KMPlYSrMF-jh"
            },
            "source": [
                "For instance, for classification problems like a spam detector, you would feed the model training data with some data labeled as 'spam', and others as 'ham' (not spam). On the other hand, for regression problems like predicting housing prices, you would instead pass in some housing data including the associated house price. In both cases, the training data you feed into your model includes the desired solutions (i.e. spam/ham, house price). "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "m23u6cjpF-ji"
            },
            "source": [
                "However, what if we are interested in something that isn't included as a target variable in the dataset? Then we need to rely on __unsupervised learning__ instead to gain insight into characteristics of the data that aren't explicity identified. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "IsPtfm3VF-ji"
            },
            "source": [
                "__Note that there are many different types of unsupervised learning algorithms, each with different use cases. For example:__\n",
                " - Clustering\n",
                "     - Hierarchical Cluster Analysis (HCA)\n",
                "     - k-Means\n",
                "     - Gaussian Mixture Models (GMMs)\n",
                "     - Expected Maximization\n",
                " - Visualization and Dimensionality Reduction\n",
                "     - Principal Component Analysis (PCA)\n",
                "     - Kernel PCA\n",
                "     - Locally Linear Embedding (LLE)\n",
                "     - t-Distibuted Stochastic Linear Embedding (t-SNE)\n",
                " - Association Rule Learning\n",
                "     - Apriori\n",
                "     - Eclat\n",
                "\n",
                "However, in this class, we shall only cover three of the clustering algorithms listed above: Hierarchical Clustering, k-Means Clustering, and Gaussian Mixture Models. Let's get started."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "vqcAUKVXF-jj"
            },
            "source": [
                "_(And as usual, if you are curious about anything data science that we mentioned, or that you've heard of outside of class, feel free to come talk to us about it after class or during office hours!)_"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "PWOkHkZQF-jj"
            },
            "source": [
                "## Hierarchical Clustering\n",
                "\n",
                "Hierarchical clustering groups observations into multiple levels of sets; the top-level set includes all of the data, and the bottom-level sets contain individual observations. The levels in between contain sets of observations with similar features. Let's take a look at a dataset. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "quqSfT13F-jk",
                "outputId": "b5ad761c-6363-406c-ffc6-2f10a98e6526",
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn import datasets\n",
                "\n",
                "iris = datasets.load_iris()\n",
                "iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
                "iris.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "ElGmNQAUF-jn"
            },
            "source": [
                "The iris dataset contains observations on three iris species. Features include sepal length, sepal width, petal length, and petal width. Notice how the species is not identified in the version of the dataset that we use above. Let's see if we can gain this information through hierarchical clustering. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "ollnqC6hF-jo"
            },
            "source": [
                "### Problem 1\n",
                "\n",
                "Perform hierarchical clustering on the iris dataset and plot the resulting clusters (as a dendrogram). How many clusters do you see? How does this compare to the number of clusters you expect to see?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "Q6io2nsOF-jo"
            },
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import StandardScaler\n",
                "from scipy.cluster.hierarchy import dendrogram, linkage\n",
                "from matplotlib import pyplot as plt\n",
                "\n",
                "iris = StandardScaler().fit_transform(iris)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "hclust = linkage(iris)\n",
                "dendrogram(hclust)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "hclust = linkage(iris, method='ward')\n",
                "dendrogram(hclust, truncate_mode='lastp', p=3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "emVIduBZF-jw"
            },
            "source": [
                "#### A side note about distances:\n",
                "To find the \"similarity\" for a group of clusters, we simply measure the magnitude of the vector between two the two data points (in whatever dimension we are working with). There are many ways to calculate the magnitude of this vector: for instance, in class we gave the example of either using the euclidean distance or the manhattan distance."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "eY2MtHEMF-jw"
            },
            "source": [
                "## K-Means Clustering\n",
                "\n",
                "K-means clustering is another technique for finding groupings in a dataset. Recall kNN from a few lectures ago; k-means uses a similar geometric approach. The k-means algorithm takes in the number of clusters (k), computes the centroid of each cluster, and assigns observations to clusters based on distance from the centroid. Let's take another look at the iris dataset. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "14lbRaZlF-jx",
                "outputId": "205fd86e-7b8e-4b0c-ca2b-a1c9493cf629"
            },
            "outputs": [],
            "source": [
                "data = datasets.load_iris()\n",
                "iris = pd.DataFrame(data.data, columns=data.feature_names)\n",
                "iris_target = pd.DataFrame(data.target, columns=['Species'])\n",
                "iris_with_target = pd.concat([iris, iris_target], axis=1)\n",
                "\n",
                "iris = iris.drop('sepal length (cm)', axis=1)\n",
                "iris = iris.drop('sepal width (cm)', axis=1)\n",
                "iris_with_target = iris_with_target.drop('sepal length (cm)', axis=1)\n",
                "iris_with_target = iris_with_target.drop('sepal width (cm)', axis=1)\n",
                "\n",
                "iris_with_target.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "JzbemfNyF-jz"
            },
            "source": [
                "We've added species identification back into the dataset for the purpose of viewing the groupings in the data. We also drop sepal length and sepal width so that we can plot the observations on a 2d graph. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "QMPYrsvxF-j0",
                "outputId": "0185bf52-c9c1-4b4e-c5cc-406d80359b97"
            },
            "outputs": [],
            "source": [
                "k = len(iris_with_target['Species'].unique())\n",
                "for i in iris_with_target['Species'].unique():\n",
                "    # select only the applicable rows\n",
                "    ds = iris_with_target[iris_with_target['Species'] == i]\n",
                "    # plot the points\n",
                "    plt.plot(ds[['petal length (cm)']],ds[['petal width (cm)']],'o')\n",
                "plt.title(\"Iris by Species\")\n",
                "plt.xlabel('petal length (cm)')\n",
                "plt.ylabel('petal width (cm)')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "oBdPc7WZF-j2"
            },
            "source": [
                "We can clearly see the three groupings in the data corresponding to the three species. Let's see if we can replicate these clusters using the k-means algorithm. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "GfDZp0CIF-j2"
            },
            "source": [
                "### Problem 2a\n",
                "\n",
                "Perform k-means clustering on the iris dataset (using only the petal length and petal width features). We have already dropped the other features for you; you should use the dataframe named 'iris' initialized above."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "-YxA6yWYF-j3"
            },
            "outputs": [],
            "source": [
                "from sklearn import cluster"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kmeans = cluster.KMeans(n_clusters=k)\n",
                "kmeans.fit(iris)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "4hII99V6F-j7"
            },
            "source": [
                "To test your code, let's plot your results and compare them to the actual groupings found above. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "fuhwoRWVF-j8",
                "outputId": "a447edd2-5fc7-4ba6-be01-6f647372a397"
            },
            "outputs": [],
            "source": [
                "labels = kmeans.labels_\n",
                "centroids = kmeans.cluster_centers_\n",
                "\n",
                "_, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
                "\n",
                "# original graph\n",
                "for i in iris_with_target['Species'].unique():\n",
                "    # select only the applicable rows\n",
                "    ds = iris_with_target[iris_with_target['Species'] == i]\n",
                "    # plot the points\n",
                "    ax1.plot(ds[['petal length (cm)']],ds[['petal width (cm)']],'o')\n",
                "ax1.set_title(\"Iris by Species\")\n",
                "ax1.set_xlabel('petal length (cm)')\n",
                "ax1.set_ylabel('petal width (cm)')\n",
                "\n",
                "# kmeans graph\n",
                "for i in range(k):\n",
                "    # select only data observations from the applicable cluster\n",
                "    ds = iris.iloc[np.where(labels==i)]\n",
                "    # plot the data observations\n",
                "    ax2.plot(ds['petal length (cm)'],ds['petal width (cm)'],'o')\n",
                "    # plot the centroids\n",
                "    lines = plt.plot(centroids[i,0],centroids[i,1],'kx')\n",
                "ax2.set_title(\"Iris by K-Means Clustering\")\n",
                "ax2.set_xlabel('petal length (cm)')\n",
                "ax2.set_ylabel('petal width (cm)')\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Problem 2b"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we will also apply kmeans clustering on a different dataset about wines. In the section of code specified below, create a kmeans model stored in a variable named `kmeans` and train it on the wines dataset. Use the number of unique values in the 'target' column as the number of clusters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "qIvFR86AF-j-",
                "outputId": "b76f708e-f577-4c02-afb9-f200d99055af"
            },
            "outputs": [],
            "source": [
                "from sklearn import datasets\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "wine = datasets.load_wine()\n",
                "wine_df = pd.DataFrame(data= np.c_[wine['data'], wine['target']],\n",
                "                     columns= wine['feature_names'] + ['target'])\n",
                "\n",
                "X = wine_df[['flavanoids','alcohol']]\n",
                "\n",
                "# YOU'RE CODE HERE!\n",
                "k = len(wine_df['target'].unique()) \n",
                "kmeans = cluster.KMeans(n_clusters=k) \n",
                "kmeans.fit(X) \n",
                "\n",
                "\n",
                "# END\n",
                "\n",
                "labels = kmeans.labels_\n",
                "centroids = kmeans.cluster_centers_\n",
                "\n",
                "_, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
                "\n",
                "\n",
                "# original graph\n",
                "for i in wine_df['target'].unique():\n",
                "    # select only the applicable rows\n",
                "    ds = wine_df[wine_df['target'] == i]\n",
                "    # plot the points\n",
                "    ax1.plot(ds[['flavanoids']],ds[['alcohol']],'o')\n",
                "ax1.set_title(\"Wines by Label\")\n",
                "ax1.set_xlabel('flavanoids')\n",
                "ax1.set_ylabel('alcohol')\n",
                "\n",
                "# kmeans graph\n",
                "for i in range(k):\n",
                "    # select only data observations from the applicable cluster\n",
                "    ds = X.iloc[np.where(labels==i)]\n",
                "    # plot the data observations\n",
                "    ax2.plot(ds['flavanoids'],ds['alcohol'],'o')\n",
                "    # plot the centroids\n",
                "    lines = plt.plot(centroids[i,0],centroids[i,1],'kx')\n",
                "ax2.set_title(\"Wines by Clustering\")\n",
                "ax2.set_xlabel('flavanoids')\n",
                "ax2.set_ylabel('alcohol')\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "gibJLp0eF-kA"
            },
            "source": [
                "## GMMs"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "iAcKyE1NF-kA"
            },
            "source": [
                "The k-means algorithm we just covered have a major flaw - they do not account for variance. You perhaps can recall from your stats class that the variance, visually, is related to the width of a bell shaped curve. Extending on this visual, you can imagine the variance in two-dimensions to just be the radius of a sphere.\n",
                "\n",
                "Whenever we perform k-means clustering on a model, it groups the data into clusters of circles (in 2 dimension, and hyper-spheres in higher dimensions). This works fine if your data is circular, but performs badly when it is in any shape otherwise. Here is a good visual example: https://towardsdatascience.com/gaussian-mixture-models-d13a5e915c8e.\n",
                "\n",
                "In such cases, we turn to GMMs. GMMs also have an added advantage of not giving hard assignments into clusters (which k-means does), rather giving 'soft assignments', where each data point can be generated by any of the distributions with corresponding probability.\n",
                "\n",
                "This concept may seem rather abstract right now - let's see it visually!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "CrXQnh47F-kB",
                "outputId": "2ca83808-1a9e-411f-ecc2-538d2f76e7f2"
            },
            "outputs": [],
            "source": [
                "from sklearn.datasets.samples_generator import make_blobs\n",
                "from scipy.spatial.distance import cdist\n",
                "\n",
                "X, y = make_blobs(n_samples=400, centers=4, cluster_std=0.60, random_state=0)\n",
                "X = X[:, ::-1] # reversing the axis for better display\n",
                "\n",
                "model = cluster.KMeans(4, random_state=0)\n",
                "model.fit(X)\n",
                "predictions = model.predict(X)\n",
                "\n",
                "ax = plt.gca()\n",
                "ax.scatter(X[:, 0], X[:, 1], c=predictions, s=40, cmap='viridis', zorder=2)\n",
                "centers = model.cluster_centers_\n",
                "radii = [cdist(X[predictions == i], [center]).max()\n",
                "         for i, center in enumerate(centers)]\n",
                "for c, r in zip(centers, radii):\n",
                "    ax.add_patch(plt.Circle(c, r, fc='#CCCCCC', lw=3, alpha=0.5, zorder=1))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "LBimblyZF-kC",
                "outputId": "44751369-4c28-4f95-9b87-4c2de0288be1"
            },
            "outputs": [],
            "source": [
                "rng = np.random.RandomState(13)\n",
                "X_stretched = np.dot(X, rng.randn(2, 2))\n",
                "\n",
                "model = cluster.KMeans(n_clusters=4, random_state=0)\n",
                "model.fit(X_stretched)\n",
                "predictions = model.predict(X_stretched)\n",
                "\n",
                "ax = plt.gca()\n",
                "ax.scatter(X_stretched[:, 0], X_stretched[:, 1], c=predictions, s=40, cmap='viridis', zorder=2)\n",
                "centers = model.cluster_centers_\n",
                "radii = [cdist(X_stretched[predictions == i], [center]).max()\n",
                "         for i, center in enumerate(centers)]\n",
                "for c, r in zip(centers, radii):\n",
                "    ax.add_patch(plt.Circle(c, r, fc='#CCCCCC', lw=3, alpha=0.5, zorder=1))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "cQYEV88IF-kE"
            },
            "source": [
                "As you can see, a K-Means model generally performs quite poorly when fitting any non-circular shapes. Let's see how a GMM performs instead:"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "gz4P3NNPF-kE"
            },
            "source": [
                "### Problem 3 (Optional)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In the cell below, you can see code for how to train a GMM and plot the points while coloring them based on the prediction of the GMM. In the following cell, we provide functions that do the exact same thing, but also plot the points with a shaded ellipse in the background corresponding to the covariance of that group. Call the `plot_gmm` function on both the `X` and `X_stretched` datasets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "bd0WWfYSF-kF",
                "outputId": "cfe5f045-357f-4b76-ba50-c62952b4dea6",
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "from sklearn.mixture import GaussianMixture\n",
                "\n",
                "gmm = GaussianMixture(n_components=4, random_state=42)\n",
                "gmm.fit(X)\n",
                "predictions = gmm.predict(X)\n",
                "plt.scatter(X[:, 0], X[:, 1], c=predictions, s=40, cmap='viridis');"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "R_D81jRHF-kG"
            },
            "source": [
                "We can see the GMMs' performance more clearly with the drawn boundaries below:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "B6Im1a26F-kH"
            },
            "outputs": [],
            "source": [
                "# you can use these functions\n",
                "from matplotlib.patches import Ellipse\n",
                "\n",
                "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
                "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
                "    ax = ax or plt.gca()\n",
                "\n",
                "    # Convert covariance to principal axes\n",
                "    if covariance.shape == (2, 2):\n",
                "        U, s, Vt = np.linalg.svd(covariance)\n",
                "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
                "        width, height = 2 * np.sqrt(s)\n",
                "    else:\n",
                "        angle = 0\n",
                "        width, height = 2 * np.sqrt(covariance)\n",
                "    \n",
                "    # Draw the Ellipse\n",
                "    for nsig in range(1, 4):\n",
                "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
                "                             angle, **kwargs))\n",
                "        \n",
                "def plot_gmm(model, X, ax=None):\n",
                "    \n",
                "    ax = ax or plt.gca()\n",
                "    model.fit(X)\n",
                "    predictions = model.predict(X)\n",
                "    ax.scatter(X[:, 0], X[:, 1], c=predictions, s=40, cmap='viridis', zorder=2)\n",
                "    ax.axis('equal')\n",
                "    \n",
                "    w_factor = 0.2 / model.weights_.max()\n",
                "    for pos, covar, w in zip(model.means_, model.covariances_, model.weights_):\n",
                "        draw_ellipse(pos, covar, alpha=w * w_factor)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gmm = GaussianMixture(n_components=4, random_state=42)\n",
                "plot_gmm(gmm, X)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gmm = GaussianMixture(n_components=4, covariance_type='full', random_state=42)\n",
                "plot_gmm(gmm, X_stretched)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "WuWJG53dF-kN"
            },
            "source": [
                "## PCA (Optional)\n",
                "\n",
                "We use principal component analysis to determine which features contribute most significantly to variation in a dataset. Let's do an example on HR analytics data. If you're interested, you can read about the dataset [here](https://www.kaggle.com/giripujar/hr-analytics)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "rZqK7RFVF-kN",
                "outputId": "a7ac3113-d313-4f7a-b76f-cd7d092d69e9"
            },
            "outputs": [],
            "source": [
                "from sklearn import preprocessing\n",
                "from sklearn.decomposition import PCA\n",
                "from matplotlib import pyplot as plt\n",
                "\n",
                "hr_data = pd.read_csv('lecture9data.csv')\n",
                "\n",
                "hr_data = hr_data.drop('left', axis=1)\n",
                "hr_data = hr_data.drop('Department', axis=1)\n",
                "hr_data = hr_data.drop('salary', axis=1)\n",
                "\n",
                "hr_data = pd.DataFrame(preprocessing.scale(hr_data),columns = hr_data.columns)\n",
                "hr_data.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "vQxAsq2NF-kP",
                "outputId": "23d108e1-0fe8-4d8a-f23e-b4aa4157ee45"
            },
            "outputs": [],
            "source": [
                "# perform PCA\n",
                "pca = PCA().fit(hr_data)\n",
                "\n",
                "# plot fraction of variance explained by each component\n",
                "x = np.arange(7)\n",
                "plt.bar(x, pca.explained_variance_ratio_)\n",
                "plt.title('Fraction of Variance Explained by Each Component')\n",
                "plt.xlabel('Component')\n",
                "plt.ylabel('Fraction of Total Variance')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "xU65A4s-F-kQ"
            },
            "source": [
                "Now we need to determine the relationship between components and features. It's important to understand that a component does not correspond to a single feature; a component may be related to multiple features. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "KjpZPqd1F-kR",
                "outputId": "a8ab37a2-128d-4f08-dfa5-192046b426a8"
            },
            "outputs": [],
            "source": [
                "components = pd.DataFrame(pca.components_, columns=hr_data.columns)\n",
                "components"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "8ajd6y3eF-kS"
            },
            "source": [
                "Each number is the correlation between the feature and the component. For example, we can see that the feature which is most strongly correlated with component 1 is satisfaction_level. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "-ARZQQueF-kT"
            },
            "source": [
                "### Question 4 (Advanced - Optional)\n",
                "\n",
                "Based on the results of the principal component analysis above, which features would you expect are most important to include in a predictive model? Why?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "PGlcDryUF-kT"
            },
            "source": [
                "satisfaction_level, last_evaluation, number_project, and average_montly_hours should be included because they have the greatest correlation with the first 2 PCA components (which are the most important ones)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "-ukLF1w6F-kT"
            },
            "source": [
                "### Further Readings:\n",
                "\n",
                "Actually, there are four major categories of machine learning systems: supervised learning, unsupervised learning, semisupervised learning, and reinforcement learning. Semisupervised learning, as you can guess, involved models that take in __partially labeled data__ (usually a lot of unlabeled data and just a little labeled data), while reinforcement learning covers goal-oriented algorithms that repeatedly learn and master a particular (usuallly complex task). Reinforcement learning is often the basis of many \"AI learns to play...\" youtube and facebook videos.\n",
                "\n",
                "_Side note, you can watch people use reinforcement learning to beat games_ [_here_](https://www.youtube.com/watch?v=L4KBBAwF_bE) _(super cool) and_ [_here_](https://www.youtube.com/watch?v=MasxAN-xZIU) _(a less successful - but no less cool - attempt)_\n",
                "\n",
                "To get an overview of these different categories, we highly recommend you read the very first chapter of this book: [__Hands-On Machine Learning with Scikit-Learn & Tensorflow__](http://index-of.es/Varios-2/Hands%20on%20Machine%20Learning%20with%20Scikit%20Learn%20and%20Tensorflow.pdf) (and continue reading if you want to learn how to implement them in Python). In fact, if you go through the book, some of the concepts should seem familiar to you, as we covered them either briefly or somewhat in depth throughout the semester. Indeed, be proud of yourself: you already have the basic concepts of machine learning and data science more or less nailed down!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "A7Mr7gLhF-kV"
            },
            "source": [
                "____"
            ]
        }
    ]
}