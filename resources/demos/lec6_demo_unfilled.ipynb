{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DHY9z0cMQmg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wine = load_wine()\n",
        "X = pd.DataFrame(data = wine.data, columns = wine.feature_names)\n",
        "Y = pd.DataFrame(data = wine.target, columns = ['target'])\n",
        "df = pd.concat([X,Y], axis=1)\n",
        "df"
      ],
      "metadata": {
        "id": "bR4X_uSQJ8oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = df[['proline','od280/od315_of_diluted_wines','target']]\n",
        "data"
      ],
      "metadata": {
        "id": "N0JLP8E0Oegg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[['proline','od280/od315_of_diluted_wines']]\n",
        "Y = data['target']\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = '' # TODO\n",
        "print(x_train)"
      ],
      "metadata": {
        "id": "lMTKR-9NTz65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = '' # TODO\n",
        "# fit\n",
        "\n",
        "# predictions\n"
      ],
      "metadata": {
        "id": "7E2ip3BSUfEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = '' # TODO\n",
        "print('Accuracy Score: ', accuracy)"
      ],
      "metadata": {
        "id": "F-hmDzZaU3fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How can we improve our KNN classifier?\n",
        "- Let's plot our data, and see what it looks like.\n",
        "- Maybe we can scale or normalize the data such that KNN assumptions are more applicable?"
      ],
      "metadata": {
        "id": "15Bu_9xK5LI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(data['proline'],data['od280/od315_of_diluted_wines'],c=data['target'].values.ravel())\n",
        "plt.xlim(0,2000) # some random large number here\n",
        "plt.ylim(0,2000)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u3G1dssFPyJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_data = data.copy()\n",
        "\n",
        "min_proline = scaled_data['proline'].min()\n",
        "max_proline = scaled_data['proline'].max()\n",
        "min_conc = scaled_data['od280/od315_of_diluted_wines'].min()\n",
        "max_conc = scaled_data['od280/od315_of_diluted_wines'].max()\n",
        "\n",
        "# performing Min Max Scaling / Min Max Normalization\n",
        "# all normalized values are in the range from 0 to 1\n",
        "scaled_data['proline'] = '' # TODO\n",
        "scaled_data['od280/od315_of_diluted_wines'] = ''  # TODO\n",
        "scaled_data"
      ],
      "metadata": {
        "id": "DYEzvXMaQZdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(scaled_data['proline'],scaled_data['od280/od315_of_diluted_wines'],c=scaled_data['target'].values.ravel())\n",
        "plt.xlim(0,1)\n",
        "plt.ylim(0,1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HRJGGNAiRTn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = scaled_data[['proline','od280/od315_of_diluted_wines']]\n",
        "Y = scaled_data['target']\n",
        "\n",
        "x_train, x_test, y_train, y_test = '' # TODO\n",
        "\n",
        "model = '' # TODO\n",
        "\n",
        "# fit\n",
        "\n",
        "# predictions\n",
        "\n",
        "\n",
        "scaled_accuracy = '' # TODO\n",
        "print('Scaled Accuracy Score: ', scaled_accuracy)"
      ],
      "metadata": {
        "id": "fuGKIJPoR9UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What about other types of data scaling?"
      ],
      "metadata": {
        "id": "g-XFwNfh6pj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " from sklearn.preprocessing import StandardScaler\n",
        "# StandardScaler follows Standard Normal Distribution (SND). It makes mean = 0 and scales the data to unit variance.\n",
        "\n",
        "scaler = '' # TODO\n",
        "standard_scaled_data = '' # TODO\n",
        "standard_scaled_data = pd.DataFrame(standard_scaled_data, columns = ['proline','od280/od315_of_diluted_wines'])\n",
        "standard_scaled_data"
      ],
      "metadata": {
        "id": "xn5t1BEUVI8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(standard_scaled_data['proline'],standard_scaled_data['od280/od315_of_diluted_wines'],c=scaled_data['target'].values.ravel())\n",
        "plt.xlim(-3,3)\n",
        "plt.ylim(-3,3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-Ayj62dLNUET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Can use sklearn.preprocessing.MinMaxScaler to replicate what we manually did above (0 = min, 1 = max)\n",
        "# MinMaxScaler scales all the data features in the range [0, 1] or else in the range [-1, 1] if there are negative values in the dataset."
      ],
      "metadata": {
        "id": "0E18xPsaPk76"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}